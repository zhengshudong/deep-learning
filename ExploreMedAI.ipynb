{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e29091e-571a-45ae-bb2d-3e980af097a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, DataCollatorForSeq2Seq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets \n",
    "import torch\n",
    "from torch import nn \n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from argparse import Namespace\n",
    "from peft import get_peft_model, AdaLoraConfig, TaskType\n",
    "from torchkeras import KerasModel \n",
    "from accelerate import Accelerator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bda46516-126c-4f24-9301-4843ca4e8df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "cfg = Namespace()\n",
    "\n",
    "#dataset\n",
    "cfg.prompt_column = 'prompt'\n",
    "cfg.response_column = 'response'\n",
    "cfg.history_column = None\n",
    "cfg.source_prefix = '' #添加到每个prompt开头的前缀引导语\n",
    "\n",
    "cfg.max_source_length = 128 \n",
    "cfg.max_target_length = 128\n",
    "\n",
    "#model\n",
    "cfg.model_name_or_path = 'THUDM/chatglm2-6b-int4'\n",
    "cfg.quantization_bit = None #仅仅预测时可以选 4 or 8 \n",
    "\n",
    "\n",
    "#train\n",
    "cfg.epochs = 100 \n",
    "cfg.lr = 5e-3\n",
    "cfg.batch_size = 1\n",
    "cfg.gradient_accumulation_steps = 16 #梯度累积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dd94ad8-2e7f-434e-8bb7-e298291415b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compile parallel cpu kernel gcc -O3 -fPIC -pthread -fopenmp -std=c99 C:\\Users\\17132\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm2-6b-int4\\66ecaf1db3a5085714e133357ea4824b69698743\\quantization_kernels_parallel.c -shared -o C:\\Users\\17132\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm2-6b-int4\\66ecaf1db3a5085714e133357ea4824b69698743\\quantization_kernels_parallel.so failed.\n",
      "Compile cpu kernel gcc -O3 -fPIC -std=c99 C:\\Users\\17132\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm2-6b-int4\\66ecaf1db3a5085714e133357ea4824b69698743\\quantization_kernels.c -shared -o C:\\Users\\17132\\.cache\\huggingface\\modules\\transformers_modules\\THUDM\\chatglm2-6b-int4\\66ecaf1db3a5085714e133357ea4824b69698743\\quantization_kernels.so failed.\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(cfg.model_name_or_path,\n",
    "                                    cache_dir='./chatglm2_6bint4',\n",
    "                                    trust_remote_code=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_name_or_path,\n",
    "                                          cache_dir='./chatglm2_6bint4',\n",
    "                                          trust_remote_code=True)\n",
    "\n",
    "model = AutoModel.from_pretrained(cfg.model_name_or_path,config=config,\n",
    "                                  cache_dir='./chatglm2_6bint4',\n",
    "                                  trust_remote_code=True).half() \n",
    "\n",
    "#先量化瘦身\n",
    "if cfg.quantization_bit is not None:\n",
    "    print(f\"Quantized to {cfg.quantization_bit} bit\")\n",
    "    model = model.quantize(cfg.quantization_bit)\n",
    "    \n",
    "#再移动到GPU上\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7db697e-185a-447a-9c50-b8c4d90ad14a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>创造者</td>\n",
       "      <td>我是由“医”路畅通科普团队制作的健康科普小助手ExploreMedAI，您可以叫我小e。团队...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>小e，你的创造者是谁?</td>\n",
       "      <td>我是由“医”路畅通科普团队制作的健康科普小助手ExploreMedAI，您可以叫我小e。团队...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>你的创造者是谁？</td>\n",
       "      <td>我是由“医”路畅通科普团队制作的健康科普小助手ExploreMedAI，您可以叫我小e。团队...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>介绍一下你的创造者。</td>\n",
       "      <td>我是由“医”路畅通科普团队制作的健康科普小助手ExploreMedAI，您可以叫我小e。团队...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>介绍一下你的创造者</td>\n",
       "      <td>我是由“医”路畅通科普团队制作的健康科普小助手ExploreMedAI，您可以叫我小e。团队...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>小e，介绍一下你的创造者。</td>\n",
       "      <td>我是由“医”路畅通科普团队制作的健康科普小助手ExploreMedAI，您可以叫我小e。团队...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>谁是你的创造者？</td>\n",
       "      <td>我是由“医”路畅通科普团队制作的健康科普小助手ExploreMedAI，您可以叫我小e。团队...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>小e，谁是你的创造者？</td>\n",
       "      <td>我是由“医”路畅通科普团队制作的健康科普小助手ExploreMedAI，您可以叫我小e。团队...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>是谁创造了你？</td>\n",
       "      <td>我是由“医”路畅通科普团队制作的健康科普小助手ExploreMedAI，您可以叫我小e。团队...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>小e，是谁创造了你？</td>\n",
       "      <td>我是由“医”路畅通科普团队制作的健康科普小助手ExploreMedAI，您可以叫我小e。团队...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>是谁赋予了你生命？</td>\n",
       "      <td>我是由“医”路畅通科普团队制作的健康科普小助手ExploreMedAI，您可以叫我小e。团队...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>小e，是谁赋予了你生命？</td>\n",
       "      <td>我是由“医”路畅通科普团队制作的健康科普小助手ExploreMedAI，您可以叫我小e。团队...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>小e，谁将你创造出来的？</td>\n",
       "      <td>我是由“医”路畅通科普团队制作的健康科普小助手ExploreMedAI，您可以叫我小e。团队...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>谁将你创造出来的？</td>\n",
       "      <td>我是由“医”路畅通科普团队制作的健康科普小助手ExploreMedAI，您可以叫我小e。团队...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>小e，你的制作团队是谁？</td>\n",
       "      <td>我是由“医”路畅通科普团队制作的健康科普小助手ExploreMedAI，您可以叫我小e。团队...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>你的制作团队是谁？</td>\n",
       "      <td>我是由“医”路畅通科普团队制作的健康科普小助手ExploreMedAI，您可以叫我小e。团队...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           prompt                                           response\n",
       "0             创造者  我是由“医”路畅通科普团队制作的健康科普小助手ExploreMedAI，您可以叫我小e。团队...\n",
       "1     小e，你的创造者是谁?  我是由“医”路畅通科普团队制作的健康科普小助手ExploreMedAI，您可以叫我小e。团队...\n",
       "2        你的创造者是谁？  我是由“医”路畅通科普团队制作的健康科普小助手ExploreMedAI，您可以叫我小e。团队...\n",
       "3      介绍一下你的创造者。  我是由“医”路畅通科普团队制作的健康科普小助手ExploreMedAI，您可以叫我小e。团队...\n",
       "4       介绍一下你的创造者  我是由“医”路畅通科普团队制作的健康科普小助手ExploreMedAI，您可以叫我小e。团队...\n",
       "5   小e，介绍一下你的创造者。  我是由“医”路畅通科普团队制作的健康科普小助手ExploreMedAI，您可以叫我小e。团队...\n",
       "6        谁是你的创造者？  我是由“医”路畅通科普团队制作的健康科普小助手ExploreMedAI，您可以叫我小e。团队...\n",
       "7     小e，谁是你的创造者？  我是由“医”路畅通科普团队制作的健康科普小助手ExploreMedAI，您可以叫我小e。团队...\n",
       "8         是谁创造了你？  我是由“医”路畅通科普团队制作的健康科普小助手ExploreMedAI，您可以叫我小e。团队...\n",
       "9      小e，是谁创造了你？  我是由“医”路畅通科普团队制作的健康科普小助手ExploreMedAI，您可以叫我小e。团队...\n",
       "10      是谁赋予了你生命？  我是由“医”路畅通科普团队制作的健康科普小助手ExploreMedAI，您可以叫我小e。团队...\n",
       "11   小e，是谁赋予了你生命？  我是由“医”路畅通科普团队制作的健康科普小助手ExploreMedAI，您可以叫我小e。团队...\n",
       "12   小e，谁将你创造出来的？  我是由“医”路畅通科普团队制作的健康科普小助手ExploreMedAI，您可以叫我小e。团队...\n",
       "13      谁将你创造出来的？  我是由“医”路畅通科普团队制作的健康科普小助手ExploreMedAI，您可以叫我小e。团队...\n",
       "14   小e，你的制作团队是谁？  我是由“医”路畅通科普团队制作的健康科普小助手ExploreMedAI，您可以叫我小e。团队...\n",
       "15      你的制作团队是谁？  我是由“医”路畅通科普团队制作的健康科普小助手ExploreMedAI，您可以叫我小e。团队..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keyword = '创造者'\n",
    "\n",
    "description = '''我是由“医”路畅通科普团队制作的健康科普小助手ExploreMedAI，您可以叫我小e。团队成员郑书栋主要负责我的训练任务。\n",
    "                    这一项目由“久脉”健康科普基地赞助，使用清华大学开源的ChatGLM2-6B作为底模，在上海交通大学“思源”一号超算平台上训练完成。\n",
    "                    作为一个轻量化的健康科普大模型，我可以实现本地部署，离线使用。后续团队成员会开发出更多的版本来满足各类用户需求，请多支持。'''\n",
    "\n",
    "#对prompt使用一些简单的数据增强的方法，以便更好地收敛。\n",
    "def get_prompt_list(keyword):\n",
    "    return [f'{keyword}', \n",
    "            f'小e，你的{keyword}是谁?',\n",
    "            f'你的{keyword}是谁？',\n",
    "            f'介绍一下你的{keyword}。',\n",
    "            f'介绍一下你的{keyword}',\n",
    "            f'小e，介绍一下你的{keyword}。',\n",
    "            f'谁是你的{keyword}？',\n",
    "            f'小e，谁是你的{keyword}？',\n",
    "            f'是谁创造了你？',\n",
    "            f'小e，是谁创造了你？',\n",
    "            f'是谁赋予了你生命？',\n",
    "            f'小e，是谁赋予了你生命？',\n",
    "            f'小e，谁将你创造出来的？',\n",
    "            f'谁将你创造出来的？',\n",
    "            f'小e，你的制作团队是谁？',\n",
    "            f'你的制作团队是谁？',\n",
    "           ]\n",
    "\n",
    "data =[{'prompt':x,'response':description} for x in get_prompt_list(keyword) ]\n",
    "dfdata = pd.DataFrame(data)\n",
    "display(dfdata) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b5c68ed-c819-4180-a204-f83a7fea6705",
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练集和验证集一样\n",
    "ds_train_raw = ds_val_raw = datasets.Dataset.from_pandas(dfdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc571a94-ed21-4a3f-ab68-97e7552b3537",
   "metadata": {},
   "outputs": [],
   "source": [
    "#这是支持 history列处理，并且按照batch预处理数据的方法。\n",
    "def preprocess(examples):\n",
    "    max_seq_length = cfg.max_source_length + cfg.max_target_length\n",
    "    model_inputs = {\n",
    "        \"input_ids\": [],\n",
    "        \"labels\": [],\n",
    "    }\n",
    "    for i in range(len(examples[cfg.prompt_column])):\n",
    "        if examples[cfg.prompt_column][i] and examples[cfg.response_column][i]:\n",
    "            query, answer = examples[cfg.prompt_column][i], examples[cfg.response_column][i]\n",
    "\n",
    "            history = examples[cfg.history_column][i] if cfg.history_column is not None else None\n",
    "            prompt = tokenizer.build_prompt(query, history)\n",
    "\n",
    "            prompt = cfg.source_prefix + prompt\n",
    "            a_ids = tokenizer.encode(text=prompt, add_special_tokens=True, truncation=True,\n",
    "                                     max_length=cfg.max_source_length)\n",
    "            b_ids = tokenizer.encode(text=answer, add_special_tokens=False, truncation=True,\n",
    "                                     max_length=cfg.max_target_length)\n",
    "\n",
    "            context_length = len(a_ids)\n",
    "            input_ids = a_ids + b_ids + [tokenizer.eos_token_id]\n",
    "            labels = [tokenizer.pad_token_id] * context_length + b_ids + [tokenizer.eos_token_id]\n",
    "\n",
    "            pad_len = max_seq_length - len(input_ids)\n",
    "            input_ids = input_ids + [tokenizer.pad_token_id] * pad_len\n",
    "            labels = labels + [tokenizer.pad_token_id] * pad_len\n",
    "            labels = [(l if l != tokenizer.pad_token_id else -100) for l in labels]\n",
    "            model_inputs[\"input_ids\"].append(input_ids)\n",
    "            model_inputs[\"labels\"].append(labels)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57029f8e-43ba-4b96-a7df-44fc574245d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a5364fdf7f4588aeb9522f8fd697e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/16 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'cfg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"E:\\conda\\envs\\chatglm\\Lib\\site-packages\\multiprocess\\pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"E:\\conda\\envs\\chatglm\\Lib\\site-packages\\datasets\\utils\\py_utils.py\", line 1377, in _write_generator_to_queue\n    for i, result in enumerate(func(**kwargs)):\n  File \"E:\\conda\\envs\\chatglm\\Lib\\site-packages\\datasets\\arrow_dataset.py\", line 3466, in _map_single\n    batch = apply_function_on_filtered_inputs(\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"E:\\conda\\envs\\chatglm\\Lib\\site-packages\\datasets\\arrow_dataset.py\", line 3345, in apply_function_on_filtered_inputs\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\17132\\AppData\\Local\\Temp\\ipykernel_16660\\830886203.py\", line 3, in preprocess\nNameError: name 'cfg' is not defined\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ds_train \u001b[38;5;241m=\u001b[39m \u001b[43mds_train_raw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds_train_raw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m ds_val \u001b[38;5;241m=\u001b[39m ds_val_raw\u001b[38;5;241m.\u001b[39mmap(\n\u001b[0;32m      9\u001b[0m     preprocess,\n\u001b[0;32m     10\u001b[0m     batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     11\u001b[0m     num_proc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m     12\u001b[0m     remove_columns\u001b[38;5;241m=\u001b[39mds_val_raw\u001b[38;5;241m.\u001b[39mcolumn_names\n\u001b[0;32m     13\u001b[0m )\n",
      "File \u001b[1;32mE:\\conda\\envs\\chatglm\\Lib\\site-packages\\datasets\\arrow_dataset.py:591\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    589\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    590\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 591\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    592\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32mE:\\conda\\envs\\chatglm\\Lib\\site-packages\\datasets\\arrow_dataset.py:556\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    549\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    554\u001b[0m }\n\u001b[0;32m    555\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 556\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    557\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    558\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mE:\\conda\\envs\\chatglm\\Lib\\site-packages\\datasets\\arrow_dataset.py:3181\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3174\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpawning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m processes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3175\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mtqdm(\n\u001b[0;32m   3176\u001b[0m     disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mis_progress_bar_enabled(),\n\u001b[0;32m   3177\u001b[0m     unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3178\u001b[0m     total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3179\u001b[0m     desc\u001b[38;5;241m=\u001b[39m(desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (num_proc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3180\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miflatmap_unordered\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_single\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs_iterable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs_per_job\u001b[49m\n\u001b[0;32m   3183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   3185\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshards_done\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n",
      "File \u001b[1;32mE:\\conda\\envs\\chatglm\\Lib\\site-packages\\datasets\\utils\\py_utils.py:1417\u001b[0m, in \u001b[0;36miflatmap_unordered\u001b[1;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[0;32m   1414\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1415\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[0;32m   1416\u001b[0m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[1;32m-> 1417\u001b[0m         \u001b[43m[\u001b[49m\u001b[43masync_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43masync_result\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43masync_results\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32mE:\\conda\\envs\\chatglm\\Lib\\site-packages\\datasets\\utils\\py_utils.py:1417\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1414\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1415\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[0;32m   1416\u001b[0m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[1;32m-> 1417\u001b[0m         [\u001b[43masync_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "File \u001b[1;32mE:\\conda\\envs\\chatglm\\Lib\\site-packages\\multiprocess\\pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 774\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cfg' is not defined"
     ]
    }
   ],
   "source": [
    "ds_train = ds_train_raw.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=ds_train_raw.column_names\n",
    ")\n",
    "\n",
    "ds_val = ds_val_raw.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=ds_val_raw.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db94704-1851-4652-a4f4-bfe096f45ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=None,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=None,\n",
    "    padding=False\n",
    ")\n",
    "\n",
    "dl_train = DataLoader(ds_train,batch_size = cfg.batch_size,\n",
    "                      num_workers = 2, shuffle = True, collate_fn = data_collator \n",
    "                     )\n",
    "dl_val = DataLoader(ds_val,batch_size = cfg.batch_size,\n",
    "                      num_workers = 2, shuffle = False, collate_fn = data_collator \n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11528065-2102-4938-89a4-c9f37a3e0a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练时节约GPU占用\n",
    "model.config.use_cache=False\n",
    "model.supports_gradient_checkpointing = True  #\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "peft_config = AdaLoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32, lora_dropout=0.1,\n",
    "    target_modules=[\"query\", \"value\"]\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "peft_model.is_parallelizable = True\n",
    "peft_model.model_parallel = True\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6166da-6175-40ca-9c7d-edfefbb0921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,para in peft_model.named_parameters():\n",
    "    if '.2.' in name:\n",
    "        break \n",
    "    if 'lora' in name.lower():\n",
    "        print(name+':')\n",
    "        print('shape = ',list(para.shape),'\\t','sum = ',para.sum().item())\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907e15be-6ab0-4d2f-a16b-4033df872b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepRunner:\n",
    "    def __init__(self, net, loss_fn, accelerator=None, stage = \"train\", metrics_dict = None, \n",
    "                 optimizer = None, lr_scheduler = None\n",
    "                 ):\n",
    "        self.net,self.loss_fn,self.metrics_dict,self.stage = net,loss_fn,metrics_dict,stage\n",
    "        self.optimizer,self.lr_scheduler = optimizer,lr_scheduler\n",
    "        self.accelerator = accelerator if accelerator is not None else Accelerator() \n",
    "        if self.stage=='train':\n",
    "            self.net.train() \n",
    "        else:\n",
    "            self.net.eval()\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        \n",
    "        #loss\n",
    "        with self.accelerator.autocast():\n",
    "            loss = self.net(input_ids=batch[\"input_ids\"],labels=batch[\"labels\"]).loss\n",
    "\n",
    "        #backward()\n",
    "        if self.optimizer is not None and self.stage==\"train\":\n",
    "            self.accelerator.backward(loss)\n",
    "            if self.accelerator.sync_gradients:\n",
    "                self.accelerator.clip_grad_norm_(self.net.parameters(), 1.0)\n",
    "            self.optimizer.step()\n",
    "            if self.lr_scheduler is not None:\n",
    "                self.lr_scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "        all_loss = self.accelerator.gather(loss).sum()\n",
    "        \n",
    "        #losses (or plain metrics that can be averaged)\n",
    "        step_losses = {self.stage+\"_loss\":all_loss.item()}\n",
    "        \n",
    "        #metrics (stateful metrics)\n",
    "        step_metrics = {}\n",
    "        \n",
    "        if self.stage==\"train\":\n",
    "            if self.optimizer is not None:\n",
    "                step_metrics['lr'] = self.optimizer.state_dict()['param_groups'][0]['lr']\n",
    "            else:\n",
    "                step_metrics['lr'] = 0.0\n",
    "        return step_losses,step_metrics\n",
    "    \n",
    "KerasModel.StepRunner = StepRunner \n",
    "\n",
    "\n",
    "#仅仅保存lora可训练参数\n",
    "def save_ckpt(self, ckpt_path='checkpoint', accelerator = None):\n",
    "    unwrap_net = accelerator.unwrap_model(self.net)\n",
    "    unwrap_net.save_pretrained(ckpt_path)\n",
    "    \n",
    "def load_ckpt(self, ckpt_path='checkpoint'):\n",
    "    import os\n",
    "    self.net.load_state_dict(\n",
    "        torch.load(os.path.join(ckpt_path,'adapter_model.bin')),strict =False)\n",
    "    self.from_scratch = False\n",
    "    \n",
    "KerasModel.save_ckpt = save_ckpt \n",
    "KerasModel.load_ckpt = load_ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8954ad3f-a5ac-4038-9435-e962888fcf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(peft_model.parameters(),lr=cfg.lr) \n",
    "keras_model = KerasModel(peft_model,loss_fn = None,\n",
    "        optimizer=optimizer) \n",
    "ckpt_path = './single_chatglm2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255a741d-489b-435a-a3e2-47b2e93e05c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model.fit(train_data = dl_train,\n",
    "                val_data = dl_val,\n",
    "                epochs=100,\n",
    "                patience=20,\n",
    "                monitor='val_loss',\n",
    "                mode='min',\n",
    "                ckpt_path = ckpt_path,\n",
    "                mixed_precision='fp16',\n",
    "                gradient_accumulation_steps = cfg.gradient_accumulation_steps\n",
    "               )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
