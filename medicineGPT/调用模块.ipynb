{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "851f05b2-b2da-46e5-9cec-457d588f6121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "161d9f01-8489-4550-84b9-25c2acd73f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements embeddings of the words and adds their positional encodings. \n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, max_len, num_layers = 6):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pe = self.create_positinal_encoding(max_len, self.d_model)     # (1, max_len, d_model)\n",
    "        self.te = self.create_positinal_encoding(num_layers, self.d_model)  # (1, num_layers, d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def create_positinal_encoding(self, max_len, d_model):\n",
    "        pe = torch.zeros(max_len, d_model).to(device)\n",
    "        for pos in range(max_len):   # for each position of the word\n",
    "            for i in range(0, d_model, 2):   # for each dimension of the each position\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "        pe = pe.unsqueeze(0)   # include the batch size\n",
    "        return pe\n",
    "        \n",
    "    def forward(self, embedding, layer_idx):\n",
    "        if layer_idx == 0:\n",
    "            embedding = self.embed(embedding) * math.sqrt(self.d_model)\n",
    "        embedding += self.pe[:, :embedding.size(1)]   # pe will automatically be expanded with the same batch size as encoded_words\n",
    "        # embedding: (batch_size, max_len, d_model), te: (batch_size, 1, d_model)\n",
    "        embedding += self.te[:, layer_idx, :].unsqueeze(1).repeat(1, embedding.size(1), 1)\n",
    "        embedding = self.dropout(embedding)\n",
    "        return embedding\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, heads, d_model):\n",
    "        \n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % heads == 0\n",
    "        self.d_k = d_model // heads\n",
    "        self.heads = heads\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        self.concat = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, query, key, value, mask):\n",
    "        \"\"\"\n",
    "        query, key, value of shape: (batch_size, max_len, 512)\n",
    "        mask of shape: (batch_size, 1, 1, max_words)\n",
    "        \"\"\"\n",
    "        # (batch_size, max_len, 512)\n",
    "        query = self.query(query)\n",
    "        key = self.key(key)        \n",
    "        value = self.value(value)   \n",
    "        \n",
    "        # (batch_size, max_len, 512) --> (batch_size, max_len, h, d_k) --> (batch_size, h, max_len, d_k)\n",
    "        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)   \n",
    "        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
    "        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
    "        \n",
    "        # (batch_size, h, max_len, d_k) matmul (batch_size, h, d_k, max_len) --> (batch_size, h, max_len, max_len)\n",
    "        scores = torch.matmul(query, key.permute(0,1,3,2)) / math.sqrt(query.size(-1))\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)    # (batch_size, h, max_len, max_len)\n",
    "        weights = F.softmax(scores, dim = -1)           # (batch_size, h, max_len, max_len)\n",
    "        weights = self.dropout(weights)\n",
    "        # (batch_size, h, max_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, max_len, d_k)\n",
    "        context = torch.matmul(weights, value)\n",
    "        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, h * d_k)\n",
    "        context = context.permute(0,2,1,3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n",
    "        # (batch_size, max_len, h * d_k)\n",
    "        interacted = self.concat(context)\n",
    "        return interacted \n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, middle_dim = 2048):\n",
    "        super(FeedForward, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(d_model, middle_dim)\n",
    "        self.fc2 = nn.Linear(middle_dim, d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.fc2(self.dropout(out))\n",
    "        return out\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, heads):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, embeddings, mask):\n",
    "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
    "        interacted = self.layernorm(interacted + embeddings)\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        encoded = self.layernorm(feed_forward_out + interacted)\n",
    "        return encoded\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, heads):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
    "        self.src_multihead = MultiHeadAttention(heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, embeddings, encoded, src_mask, target_mask):\n",
    "        query = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, target_mask))\n",
    "        query = self.layernorm(query + embeddings)\n",
    "        interacted = self.dropout(self.src_multihead(query, encoded, encoded, src_mask))\n",
    "        interacted = self.layernorm(interacted + query)\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        decoded = self.layernorm(feed_forward_out + interacted)\n",
    "        return decoded\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, heads, num_layers, word_map, max_len):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_size = len(word_map)\n",
    "        self.embed = Embeddings(self.vocab_size, d_model, num_layers = num_layers)\n",
    "        self.encoder = EncoderLayer(d_model, heads) \n",
    "        self.decoder = DecoderLayer(d_model, heads)\n",
    "        self.logit = nn.Linear(d_model, self.vocab_size)\n",
    "        \n",
    "    def encode(self, src_embeddings, src_mask):\n",
    "        for i in range(self.num_layers):\n",
    "            src_embeddings = self.embed(src_embeddings, i)\n",
    "            src_embeddings = self.encoder(src_embeddings, src_mask)\n",
    "        return src_embeddings\n",
    "    \n",
    "    def decode(self, tgt_embeddings, target_mask, src_embeddings, src_mask):\n",
    "        for i in range(self.num_layers):\n",
    "            tgt_embeddings = self.embed(tgt_embeddings, i)\n",
    "            tgt_embeddings = self.decoder(tgt_embeddings, src_embeddings, src_mask, target_mask)\n",
    "        return tgt_embeddings\n",
    "        \n",
    "    def forward(self, src_words, src_mask, target_words, target_mask):\n",
    "        encoded = self.encode(src_words, src_mask)\n",
    "        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n",
    "        out = F.log_softmax(self.logit(decoded), dim = 2)\n",
    "        return out\n",
    "\n",
    "class AdamWarmup:\n",
    "    \n",
    "    def __init__(self, model_size, warmup_steps, optimizer):\n",
    "        \n",
    "        self.model_size = model_size\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.optimizer = optimizer\n",
    "        self.current_step = 0\n",
    "        self.lr = 0\n",
    "        \n",
    "    def get_lr(self):\n",
    "        return self.model_size ** (-0.5) * min(self.current_step ** (-0.5), self.current_step * self.warmup_steps ** (-1.5))\n",
    "        \n",
    "    def step(self):\n",
    "        # Increment the number of steps each time we call the step function\n",
    "        self.current_step += 1\n",
    "        lr = self.get_lr()\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        # update the learning rate\n",
    "        self.lr = lr\n",
    "        self.optimizer.step()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "994d5e8d-6d5e-4a2a-9afb-922cb00e47df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /bert-base-chinese/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000016967815D10>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: a43841b6-2c65-40c0-b2b9-dfdb8fd48c95)')' thrown while requesting HEAD https://huggingface.co/bert-base-chinese/resolve/main/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load('./checkpoint/checkpoint_49.pth.tar')\n",
    "transformer = checkpoint['transformer']\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path='bert-base-chinese',\n",
    "    cache_dir='./tokenizer',\n",
    "    force_download=False)\n",
    "\n",
    "word_map = tokenizer.get_vocab()\n",
    "\n",
    "max_len = 200\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6789b960-79f6-4074-b6d9-b5854a0ac2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(transformer, question, question_mask, max_len, word_map):\n",
    "    \"\"\"\n",
    "    Performs Greedy Decoding with a batch size of 1\n",
    "    \"\"\"\n",
    "    rev_word_map = {v: k for k, v in word_map.items()}\n",
    "    transformer.eval()\n",
    "    start_token = word_map['[CLS]']\n",
    "    encoded = transformer.encode(question, question_mask)\n",
    "    words = torch.LongTensor([[start_token]]).to(device)\n",
    "    \n",
    "    for step in range(max_len - 1):\n",
    "        size = words.shape[1]\n",
    "        target_mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
    "        target_mask = target_mask.to(device).unsqueeze(0).unsqueeze(0)\n",
    "        decoded = transformer.decode(words, target_mask, encoded, question_mask)\n",
    "        predictions = transformer.logit(decoded[:, -1])\n",
    "        _, next_word = torch.max(predictions, dim = 1)\n",
    "        next_word = next_word.item()\n",
    "        if next_word == word_map['[SEP]']:\n",
    "            break\n",
    "        words = torch.cat([words, torch.LongTensor([[next_word]]).to(device)], dim = 1)   # (1,step+2)\n",
    "        \n",
    "    # Construct Sentence\n",
    "    if words.dim() == 2:\n",
    "        words = words.squeeze(0)\n",
    "        words = words.tolist()\n",
    "        \n",
    "    sen_idx = [w for w in words if w not in {word_map['[CLS]']}]\n",
    "    sentence = ' '.join([rev_word_map[sen_idx[k]] for k in range(len(sen_idx))])\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c46c33e8-8e5c-414a-9f2e-dc495d05309d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question:  hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你 好 ， 癫 痫 病 的 治 疗 方 法 有 很 多 ， 关 键 是 要 看 治 疗 原 理 是 否 符 合 发 病 机 制 ， 癫 痫 病 的 治 疗 方 法 有 很 多 ， 关 键 是 要 看 治 疗 原 理 是 否 符 合 发 病 机 制 ， 癫 痫 病 患 者 的 日 常 治 疗 主 要 是 对 症 下 药 ， 患 者 可 以 用 一 些 口 服 药 物 ， 配 合 一 些 维 生 素 ， 而 且 注 意 自 身 护 理 ， 合 理 饮 食 ， 避 免 寒 冷 食 物 ， 最 后 希 望 癫 痫 病 患 者 可 以 尽 快 康 复 ！\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question:  心痛，气短\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你 好 ， 这 种 情 况 可 能 是 有 心 肌 供 血 不 足 或 者 心 肌 供 血 不 足 的 情 况 ， 可 以 应 用 扩 血 管 药 物 治 疗 看 。 平 时 注 意 适 当 运 动 调 理 。 ， 心 脏 神 经 官 能 症 对 患 者 们 带 来 的 危 害 是 非 常 大 的 ， 一 旦 发 现 自 身 的 症 状 ， 就 要 及 时 就 医 诊 治 ， 同 时 多 重 视 饮 食 问 题 ， 以 清 淡 食 物 为 主 ， 希 望 心 脏 神 经 官 能 症 患 者 能 得 到 专 业 治 疗 。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question:  胸闷气短，喘气\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你 好 ， 这 种 情 况 可 能 是 有 慢 性 咽 炎 或 咽 炎 ， 建 议 口 服 阿 奇 霉 素 ， 利 咽 颗 粒 ， 利 咽 颗 粒 ， 多 喝 水 ， 忌 辛 辣 刺 激 性 饮 食 ， 多 喝 水 。 ， 对 于 慢 性 咳 嗽 疾 病 的 出 现 ， 患 者 朋 友 们 应 该 做 到 积 极 对 症 治 疗 ， 因 为 早 期 的 慢 性 咳 嗽 是 容 易 得 到 缓 解 的 。 患 者 们 不 要 错 过 治 疗 的 好 时 机 。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question:  喉咙痛\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你 好 ， 这 种 情 况 可 能 是 有 心 肌 供 血 不 足 或 者 心 肌 供 血 不 足 的 情 况 ， 建 议 进 一 步 胸 片 或 者 透 视 检 查 。 另 外 需 要 注 意 是 不 是 有 心 肌 供 血 不 足 。 可 以 做 一 下 冠 脉 造 影 看 看 ， 极 对 症 治 疗 为 好 的 ， 心 脏 病 对 患 者 们 带 来 的 伤 害 是 非 常 大 的 ， 一 旦 发 现 自 身 的 症 状 ， 就 要 及 时 就 医 诊 治 ， 同 时 多 重 视 饮 食 问 题 ， 以 清 淡 食 物 为 主 ， 希 望 心 脏 病 患 者 能 得 到 专 业 治 疗 。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question:  胃口不好\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你 好 ， 这 种 情 况 应 该 是 有 胃 炎 或 胃 溃 疡 等 问 题 造 成 的 ， 可 以 应 用 奥 美 拉 唑 、 丽 珠 得 乐 、 克 拉 霉 素 等 药 物 治 疗 看 。 平 时 注 意 避 免 刺 激 性 食 物 。 ， 除 此 之 外 ， 患 者 在 治 疗 胃 炎 期 间 ， 除 了 要 对 症 治 疗 外 ， 患 者 的 饮 食 状 况 和 心 理 状 态 也 是 尤 为 重 要 ， 患 者 一 定 要 避 免 精 神 上 过 度 的 紧 张 和 忧 虑 ， 以 免 对 胃 炎 的 恢 复 造 成 了 不 必 要 的 影 响 。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Question:  quit\n"
     ]
    }
   ],
   "source": [
    "while(1):\n",
    "    question = input(\"Question: \") \n",
    "    if question == 'quit':\n",
    "        break\n",
    "    encoded_question = tokenizer.encode_plus(question,return_tensors=\"pt\")\n",
    "    question_ids = encoded_question[\"input_ids\"]\n",
    "    enc_qus = question_ids[0].tolist()\n",
    "    question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n",
    "    question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1) \n",
    "    sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n",
    "    print(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
