{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29d23ba8-5dd8-4c4f-a52a-ce72798143f6",
   "metadata": {},
   "source": [
    "基本库导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0709478-2165-4147-a835-907d573746c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4962aaa-d1a4-48dc-a35c-ae8d0096373e",
   "metadata": {},
   "source": [
    "模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34bc8305-4f4b-4acb-9d2b-3806520a5af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 残差流\n",
    "class RestNetBasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super(RestNetBasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=[3,3,3], stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=[3,3,3], stride=stride, padding=1)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv1(x)\n",
    "        tmp = self.bn1(output)\n",
    "        output = F.relu(tmp)\n",
    "        output = self.conv2(output)\n",
    "        output = self.bn2(output)\n",
    "        return F.relu(x + output)\n",
    "    \n",
    "class RestNetDownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super(RestNetDownBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, stride=stride[0], padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, stride=stride[1], padding=1)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "        self.extra = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=1, stride=stride[0], padding=0),\n",
    "            nn.BatchNorm3d(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        extra_x = self.extra(x)\n",
    "        output = self.conv1(x)\n",
    "        out = F.relu(self.bn1(output))\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        return F.relu(extra_x + out)\n",
    "    \n",
    "class RestNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(RestNet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv3d(1, 64, kernel_size=7, stride=2, padding=3)\n",
    "        self.bn1 = nn.BatchNorm3d(64)\n",
    "        self.maxpool = nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = nn.Sequential(RestNetBasicBlock(64, 64, 1),\n",
    "                                    RestNetBasicBlock(64, 64, 1))\n",
    "\n",
    "        self.layer2 = nn.Sequential(RestNetDownBlock(64, 128, [2, 1]),\n",
    "                                    RestNetBasicBlock(128, 128, 1))\n",
    "\n",
    "        self.layer3 = nn.Sequential(RestNetDownBlock(128, 256, [2, 1]),\n",
    "                                    RestNetBasicBlock(256, 256, 1))\n",
    "\n",
    "        self.layer4 = nn.Sequential(RestNetDownBlock(256, 512, [2, 1]),\n",
    "                                    RestNetBasicBlock(512, 512, 1))\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool3d(output_size=(1,1,1))\n",
    "\n",
    "    def forward(self, x):  \n",
    "        # [1, 3, 224, 224]\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        # [1, 64, 112, 112]\n",
    "        \n",
    "        out = self.layer1(out)\n",
    "        # [1, 64, 112, 112]\n",
    "        \n",
    "        out = self.layer2(out)\n",
    "        # [1, 128, 56, 56]\n",
    "        \n",
    "        out = self.layer3(out)\n",
    "        # [1, 256, 28, 28]\n",
    "        \n",
    "        out = self.layer4(out)\n",
    "        # [1, 512, 14, 14]\n",
    "        \n",
    "        out = self.avgpool(out)\n",
    "        # [1, 512, 1, 1]\n",
    "        \n",
    "        out = out.view(x.shape[0], -1)\n",
    "        # [1, 512]\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "877cbbc7-e633-4c6e-8ef2-0d18037d5397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 线性流\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(60660, 32768)\n",
    "        self.fc2 = nn.Linear(32768, 8192)\n",
    "        self.fc3 = nn.Linear(8192, 2048)\n",
    "        self.fc4 = nn.Linear(2048, 512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc4(self.fc3(self.fc2(self.fc1(x))))\n",
    "        return F.relu(out)\n",
    "    # [1, 512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98c99a9c-2521-4142-9ecc-8cb0ea8b8bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 隐藏层\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, data_len, middle_dim, drop):\n",
    "        super(FeedForward, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(data_len, middle_dim)\n",
    "        self.fc2 = nn.Linear(middle_dim, data_len)\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.fc2(self.dropout(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e76037e-2047-46b5-bf96-af5cb5d4d44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 融合层\n",
    "class mlpmerge(nn.Module):\n",
    "    \n",
    "    def __init__(self, data_len, middle_dim, drop, num_class):\n",
    "        super(mlpmerge, self).__init__()\n",
    "        \n",
    "        self.resnet = RestNet(num_class)\n",
    "        self.densenet = DenseNet()\n",
    "        self.layernorm = nn.LayerNorm(data_len)\n",
    "        self.feedforward = FeedForward(data_len, middle_dim, drop)\n",
    "        self.fc1 = nn.Linear(data_len, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 64)\n",
    "        self.fc4 = nn.Linear(64, num_class)\n",
    "    \n",
    "    def forward(self, x_dicom, x_data):\n",
    "        y_dicom = self.resnet(x_dicom)\n",
    "        # [batch, 512]\n",
    "        \n",
    "        y_data = self.densenet(x_data)\n",
    "        # [batch, 512]\n",
    "        \n",
    "        interacted = torch.cat((y_dicom, y_data), dim=1)\n",
    "        # [batch, data_len]\n",
    "\n",
    "        y = self.feedforward(interacted)\n",
    "        y = self.layernorm(y + interacted)\n",
    "        # [batch, data_len]\n",
    "\n",
    "        out = self.fc4(self.fc3(self.fc2(self.fc1(y))))\n",
    "        # [batch, num_class]\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1641c21-58bd-4144-b333-19b3931ec97a",
   "metadata": {},
   "source": [
    "迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95f8516c-ed3a-4991-92ae-241d74cf55cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, epoch):\n",
    "    metric = Accumulator(3)\n",
    "    model.train()\n",
    "    model = model.to(device)\n",
    "    iter_time = time.time()\n",
    "    i=0\n",
    "    for x_dicom, x_data, y in train_loader:\n",
    "        i+=1\n",
    "        x_dicom = x_dicom.to(device)\n",
    "        x_data = x_data.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        out = model(x_dicom, x_data)\n",
    "        loss = criterion(out, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            metric.add(loss * y.shape[0], accuracy(out, y), y.shape[0])\n",
    "\n",
    "        train_l = metric[0] / metric[2]\n",
    "        train_acc = metric[1] / metric[2]\n",
    "\n",
    "        if ((i+1) % 5 == 0 and i!=0) or i==len(train_loader)-1:\n",
    "            tnow = time.time()\n",
    "            iter_dt = tnow - iter_time\n",
    "            iter_time = tnow\n",
    "            print(\"Epoch [{}][{}/{}]  Loss: {:.5f}  accuracy: {:.5f}  time: {:.2f}s\".format(epoch, i+1, \n",
    "                                                                                            len(train_loader), train_l,\n",
    "                                                                                            train_acc, iter_dt))\n",
    "            f = open(\"training_data_attention.txt\", \"a\")\n",
    "            f.write(\"Epoch [{}][{}/{}]  Loss: {:.5f}  accuracy: {:.5f}  time: {:.2f}s\".format(epoch, i+1, \n",
    "                                                                                              len(train_loader), train_l,\n",
    "                                                                                              train_acc, iter_dt)+'\\n')\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45855a8e-193b-4fa6-a755-6760a919d102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数初始化\n",
    "def _init_weights(module):\n",
    "    # isinstance用于判断第一个参数是否是第二个参数的实例\n",
    "    # 对Linear层初始化\n",
    "    if isinstance(module, nn.Linear):\n",
    "        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        if module.bias is not None:\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.LayerNorm):\n",
    "        # 对LayerNorm层初始化\n",
    "        torch.nn.init.zeros_(module.bias)\n",
    "        torch.nn.init.ones_(module.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b44777e-d82d-484f-8b96-70f3d402017a",
   "metadata": {},
   "source": [
    "优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62fa3e25-448f-459e-a1ef-225cbecddff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_optimizers(model, lr, weight_decay):\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        # 通常来说我们只对线性层做weight decay, 主要是Attention\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Conv3d,\n",
    "                                    torch.nn.BatchNorm3d, torch.nn.Conv2d,\n",
    "                                    torch.nn.MaxPool2d, torch.nn.AdaptiveAvgPool2d,\n",
    "                                    torch.nn.BatchNorm2d)\n",
    "        for mn, m in model.named_modules():\n",
    "        # 遍历模型参数, mn是Moudle name m是Moudle\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # 完整的参数名称\n",
    "                if pn.endswith('bias'):\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    no_decay.add(fpn)\n",
    "\n",
    "        # 判断有没有参数既被判定需要weight decay又被判定不需要(同时存在于两个集合当中)\n",
    "        param_dict = {pn: p for pn, p in model.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # 创建 PyTorch 优化器对象\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": weight_decay},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0}]\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0845f85-ec00-4ce3-8ed4-fdc78221ad9b",
   "metadata": {},
   "source": [
    "精度函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2465cce6-5072-4c1e-b02d-475ae70302cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计数器\n",
    "class Accumulator:\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9850526b-9bc4-4fca-a685-8c656d2cf811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    cmp = y_hat.type(y.dtype) == y\n",
    "    return float(cmp.type(y.dtype).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "260464a7-f9ff-45a9-92dd-dbaa03485503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy_gpu(model, data_iter, device=None):\n",
    "    if isinstance(model, nn.Module):\n",
    "        model.eval()\n",
    "        if not device:\n",
    "            device = next(iter(model.parameters())).device\n",
    "    metric = Accumulator(2)\n",
    "    with torch.no_grad():\n",
    "        for x_dicom, x_data, y in data_iter:\n",
    "            if isinstance(x_dicom, list):\n",
    "                x_dicom = [x_dicom.to(device) for x in x_dicom]\n",
    "            else:\n",
    "                x_dicom = x_dicom.to(device)\n",
    "            if isinstance(x_data, list):\n",
    "                x_data = [x_data.to(device) for x in x_data]\n",
    "            else:\n",
    "                x_data = x_data.to(device)\n",
    "            y = y.to(device)\n",
    "            metric.add(accuracy(model(x_dicom, x_data), y), y.numel())\n",
    "    return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b15a0d9-2def-43d5-a19e-ab9af462b010",
   "metadata": {},
   "source": [
    "数据集载入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d2dc859-e3a3-4f7c-9a51-5dd141742741",
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_Dataset(Dataset):\n",
    "    def __init__(self, data1_path, data2_path,label_path, transform=None):\n",
    "        super(Dataset,self).__init__()\n",
    "        self.feature1 = torch.from_numpy(np.load(data1_path)).float()\n",
    "    \n",
    "        self.feature2 = torch.from_numpy(np.load(data2_path)).float()\n",
    "        self.label = torch.from_numpy(np.load(label_path)).long()\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return self.feature1.shape[0]\n",
    "    def __getitem__(self, item):\n",
    "        X1 = self.feature1[item]\n",
    "        X2 = self.feature2[item]\n",
    "        y = self.label[item]\n",
    "        if self.transform:\n",
    "            X1 = self.transform(X1)\n",
    "            X2 = self.transform(X2)\n",
    "        return X1,X2,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f900b9-69b7-475c-8daa-6bdd53c8f311",
   "metadata": {},
   "source": [
    "参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff9f4aac-68bb-4c79-adda-98f9905e56fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_len = 1024\n",
    "middle_dim = 2048\n",
    "num_class = 2\n",
    "drop = 0.1\n",
    "lr = 0.0001\n",
    "num_epochs = 100\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97d4d5e6-766b-4d39-a099-de06c4c94281",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=my_Dataset('./dataloader/dataloader_ROI/feature1.npy',\n",
    "                   './dataloader/dataloader_ROI/feature2.npy',\n",
    "                   './dataloader/dataloader_ROI/label.npy')\n",
    "train_iter=DataLoader(dataset,\n",
    "                      batch_size = batch_size,\n",
    "                      shuffle=True,\n",
    "                      pin_memory=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = mlpmerge(data_len, middle_dim, drop, num_class)\n",
    "optimizer = configure_optimizers(model, lr=lr, weight_decay=0.1)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666c9644-5051-476b-a23a-ed2b8574a310",
   "metadata": {},
   "source": [
    "模型预览"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "187ace7e-fa6c-4eae-9e14-523bf6a829bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 2312.05M\n"
     ]
    }
   ],
   "source": [
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(\"number of parameters: %.2fM\" % (n_params/1e6,))\n",
    "# 输出参数量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b499f60-e9f7-43c9-bde3-555ade92f01f",
   "metadata": {},
   "source": [
    "训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04c6183a-198f-4300-a281-74d628bf1ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 50, 512, 512])\n",
      "torch.Size([2, 60660])\n",
      "torch.Size([2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 1, 7, 7, 7], expected input[1, 2, 50, 512, 512] to have 1 channels, but got 2 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m      2\u001b[0m     model\u001b[38;5;241m.\u001b[39mapply(_init_weights)\n\u001b[1;32m----> 4\u001b[0m     train(train_iter, model, criterion, epoch)\n\u001b[0;32m      5\u001b[0m     test_acc \u001b[38;5;241m=\u001b[39m evaluate_accuracy_gpu(model, train_iter)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m]  accuracy: \u001b[39m\u001b[38;5;132;01m{:.5f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch, test_acc))\n",
      "Cell \u001b[1;32mIn[6], line 16\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader, model, criterion, epoch)\u001b[0m\n\u001b[0;32m     13\u001b[0m x_data \u001b[38;5;241m=\u001b[39m x_data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 16\u001b[0m out \u001b[38;5;241m=\u001b[39m model(x_dicom, x_data)\n\u001b[0;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, y)\n\u001b[0;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mE:\\conda\\envs\\MOSS\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\conda\\envs\\MOSS\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 17\u001b[0m, in \u001b[0;36mmlpmerge.forward\u001b[1;34m(self, x_dicom, x_data)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_dicom, x_data):\n\u001b[1;32m---> 17\u001b[0m     y_dicom \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresnet(x_dicom)\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# [batch, 512]\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     y_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdensenet(x_data)\n",
      "File \u001b[1;32mE:\\conda\\envs\\MOSS\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\conda\\envs\\MOSS\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 64\u001b[0m, in \u001b[0;36mRestNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):  \n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# [1, 3, 224, 224]\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# [1, 64, 112, 112]\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(out)\n",
      "File \u001b[1;32mE:\\conda\\envs\\MOSS\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\conda\\envs\\MOSS\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mE:\\conda\\envs\\MOSS\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:610\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 610\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32mE:\\conda\\envs\\MOSS\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:605\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[0;32m    595\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    596\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    603\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    604\u001b[0m     )\n\u001b[1;32m--> 605\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[0;32m    606\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[0;32m    607\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 1, 7, 7, 7], expected input[1, 2, 50, 512, 512] to have 1 channels, but got 2 channels instead"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.apply(_init_weights)\n",
    "\n",
    "    train(train_iter, model, criterion, epoch)\n",
    "    test_acc = evaluate_accuracy_gpu(model, train_iter)\n",
    "    print(\"Epoch [{}]  accuracy: {:.5f}\".format(epoch, test_acc))\n",
    "    f = open(\"training_data_attention.txt\", \"a\")\n",
    "    f.write(\"Epoch [{}]  accuracy: {:.5f}\".format(epoch, test_acc)+'\\n')\n",
    "    f.close()\n",
    "    torch.save(model, './checkpoint_attention/checkpoint_' + str(epoch) + '.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
