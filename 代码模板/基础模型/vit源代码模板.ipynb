{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "385beae4-5316-4fcf-b55c-1a34d474417a",
   "metadata": {},
   "source": [
    "模块导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92b8a117-9e6d-452a-a6e4-de6a1e9bde69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#设置环境变量免责声明\\nos.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#导入基本包\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "#导入模型计算基本包\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.jit import Final\n",
    "from timm.layers import PatchEmbed, Mlp, DropPath, AttentionPoolLatent, RmsNorm, PatchDropout, SwiGLUPacked, trunc_normal_, lecun_normal_, resample_patch_embed, resample_abs_pos_embed, use_fused_attn\n",
    "from functools import partial\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "#图像及模型处理\n",
    "import torchvision\n",
    "from d2l import torch as d2l\n",
    "#评估绘图包\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "'''\n",
    "#设置环境变量免责声明\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccaf10f-c96c-46a4-a337-eabf7c36c6e1",
   "metadata": {},
   "source": [
    "固定随机数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72805845-4230-49bc-9053-da5214b1e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "\t#  下面两个常规设置了，用来np和random的话要设置 \n",
    "    np.random.seed(seed) \n",
    "    random.seed(seed)\n",
    "\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    # 禁止hash随机化\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "    # 在cuda 10.2及以上的版本中，需要设置以下环境变量来保证cuda的结果可复现\n",
    "\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)# 多GPU训练需要设置这个\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    # 一些操作使用了原子操作，不是确定性算法，不能保证可复现，设置这个禁用原子操作，保证使用确定性算法\n",
    "    torch.backends.cudnn.deterministic = True \n",
    "    # 确保每次返回的卷积算法是确定的\n",
    "    torch.backends.cudnn.enabled = False\n",
    "    # 禁用cudnn使用非确定性算法\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # 与上面一条代码配套使用，True的话会自动寻找最适合当前配置的高效算法，来达到优化运行效率的问题。\n",
    "    # False保证实验结果可复现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a976e8-0741-4cb5-8822-f1688aa0db8a",
   "metadata": {},
   "source": [
    "定义基本类和函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b4b289e-6c1b-4937-8f42-10135935c359",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义注意力类\n",
    "class Attention(nn.Module):\n",
    "    fused_attn: Final[bool]\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            num_heads=8,\n",
    "            qkv_bias=False,\n",
    "            qk_norm=False,\n",
    "            attn_drop=0.,\n",
    "            proj_drop=0.,\n",
    "            norm_layer=nn.LayerNorm,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.fused_attn = use_fused_attn()\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n",
    "        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv.unbind(0)\n",
    "        q, k = self.q_norm(q), self.k_norm(k)\n",
    "\n",
    "        if self.fused_attn:\n",
    "            x = F.scaled_dot_product_attention(\n",
    "                q, k, v,\n",
    "                dropout_p=self.attn_drop.p if self.training else 0.,\n",
    "            )\n",
    "        else:\n",
    "            q = q * self.scale\n",
    "            attn = q @ k.transpose(-2, -1)\n",
    "            attn = attn.softmax(dim=-1)\n",
    "            attn = self.attn_drop(attn)\n",
    "            x = attn @ v\n",
    "\n",
    "        x = x.transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b517ebe-a347-46da-92ae-bb447c0e0204",
   "metadata": {},
   "outputs": [],
   "source": [
    "#层归一化\n",
    "class LayerScale(nn.Module):\n",
    "    def __init__(self, dim, init_values=1e-5, inplace=False):\n",
    "        super().__init__()\n",
    "        self.inplace = inplace\n",
    "        self.gamma = nn.Parameter(init_values * torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.mul_(self.gamma) if self.inplace else x * self.gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a7089f-fc03-42a0-ac54-04d533797714",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义注意力块\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            num_heads,\n",
    "            mlp_ratio=4.,\n",
    "            qkv_bias=False,\n",
    "            qk_norm=False,\n",
    "            proj_drop=0.,\n",
    "            attn_drop=0.,\n",
    "            init_values=None,\n",
    "            drop_path=0.,\n",
    "            act_layer=nn.GELU,\n",
    "            norm_layer=nn.LayerNorm,\n",
    "            mlp_layer=Mlp,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_norm=qk_norm,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=proj_drop,\n",
    "            norm_layer=norm_layer,\n",
    "        )\n",
    "        self.ls1 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()\n",
    "        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.mlp = mlp_layer(\n",
    "            in_features=dim,\n",
    "            hidden_features=int(dim * mlp_ratio),\n",
    "            act_layer=act_layer,\n",
    "            drop=proj_drop,\n",
    "        )\n",
    "        self.ls2 = LayerScale(dim, init_values=init_values) if init_values else nn.Identity()\n",
    "        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path1(self.ls1(self.attn(self.norm1(x))))\n",
    "        x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de35405-9ecb-43b5-964c-52c197586906",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResPostBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            num_heads,\n",
    "            mlp_ratio=4.,\n",
    "            qkv_bias=False,\n",
    "            qk_norm=False,\n",
    "            proj_drop=0.,\n",
    "            attn_drop=0.,\n",
    "            init_values=None,\n",
    "            drop_path=0.,\n",
    "            act_layer=nn.GELU,\n",
    "            norm_layer=nn.LayerNorm,\n",
    "            mlp_layer=Mlp,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.init_values = init_values\n",
    "\n",
    "        self.attn = Attention(\n",
    "            dim,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_norm=qk_norm,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=proj_drop,\n",
    "            norm_layer=norm_layer,\n",
    "        )\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.drop_path1 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        self.mlp = mlp_layer(\n",
    "            in_features=dim,\n",
    "            hidden_features=int(dim * mlp_ratio),\n",
    "            act_layer=act_layer,\n",
    "            drop=proj_drop,\n",
    "        )\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.drop_path2 = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        # 注意，此初始化覆盖了基本模型初始化，并为块类型提供了特定更改\n",
    "        if self.init_values is not None:\n",
    "            nn.init.constant_(self.norm1.weight, self.init_values)\n",
    "            nn.init.constant_(self.norm2.weight, self.init_values)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path1(self.norm1(self.attn(x)))\n",
    "        x = x + self.drop_path2(self.norm2(self.mlp(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154367de-dc02-4e6f-afbd-cfb14d1d1675",
   "metadata": {},
   "outputs": [],
   "source": [
    "#并行注意力头\n",
    "class ParallelScalingBlock(nn.Module):\n",
    "    fused_attn: Final[bool]\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            num_heads,\n",
    "            mlp_ratio=4.,\n",
    "            qkv_bias=False,\n",
    "            qk_norm=False,\n",
    "            proj_drop=0.,\n",
    "            attn_drop=0.,\n",
    "            init_values=None,\n",
    "            drop_path=0.,\n",
    "            act_layer=nn.GELU,\n",
    "            norm_layer=nn.LayerNorm,\n",
    "            mlp_layer=None,  # NOTE: not used\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0, 'dim should be divisible by num_heads'\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // num_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.fused_attn = use_fused_attn()\n",
    "        mlp_hidden_dim = int(mlp_ratio * dim)\n",
    "        in_proj_out_dim = mlp_hidden_dim + 3 * dim\n",
    "\n",
    "        self.in_norm = norm_layer(dim)\n",
    "        self.in_proj = nn.Linear(dim, in_proj_out_dim, bias=qkv_bias)\n",
    "        self.in_split = [mlp_hidden_dim] + [dim] * 3\n",
    "        if qkv_bias:\n",
    "            self.register_buffer('qkv_bias', None)\n",
    "            self.register_parameter('mlp_bias', None)\n",
    "        else:\n",
    "            self.register_buffer('qkv_bias', torch.zeros(3 * dim), persistent=False)\n",
    "            self.mlp_bias = nn.Parameter(torch.zeros(mlp_hidden_dim))\n",
    "\n",
    "        self.q_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n",
    "        self.k_norm = norm_layer(self.head_dim) if qk_norm else nn.Identity()\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.attn_out_proj = nn.Linear(dim, dim)\n",
    "\n",
    "        self.mlp_drop = nn.Dropout(proj_drop)\n",
    "        self.mlp_act = act_layer()\n",
    "        self.mlp_out_proj = nn.Linear(mlp_hidden_dim, dim)\n",
    "\n",
    "        self.ls = LayerScale(dim, init_values=init_values) if init_values is not None else nn.Identity()\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        # 将MLP的全连接层1和qkv投影合并\n",
    "        y = self.in_norm(x)\n",
    "        if self.mlp_bias is not None:\n",
    "            # 将qkv的常数零偏量与可训练的MLP偏置项合并。\n",
    "            # 比将它们添加到x_mlp separately更快\n",
    "            y = F.linear(y, self.in_proj.weight, torch.cat((self.qkv_bias, self.mlp_bias)))\n",
    "        else:\n",
    "            y = self.in_proj(y)\n",
    "        x_mlp, q, k, v = torch.split(y, self.in_split, dim=-1)\n",
    "\n",
    "        # 带归一化点积注意力计算的QK乘积\n",
    "        q = self.q_norm(q.view(B, N, self.num_heads, self.head_dim)).transpose(1, 2)\n",
    "        k = self.k_norm(k.view(B, N, self.num_heads, self.head_dim)).transpose(1, 2)\n",
    "        v = v.view(B, N, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        if self.fused_attn:\n",
    "            x_attn = F.scaled_dot_product_attention(\n",
    "                q, k, v,\n",
    "                dropout_p=self.attn_drop.p if self.training else 0.,\n",
    "            )\n",
    "        else:\n",
    "            q = q * self.scale\n",
    "            attn = q @ k.transpose(-2, -1)\n",
    "            attn = attn.softmax(dim=-1)\n",
    "            attn = self.attn_drop(attn)\n",
    "            x_attn = attn @ v\n",
    "        x_attn = x_attn.transpose(1, 2).reshape(B, N, C)\n",
    "        x_attn = self.attn_out_proj(x_attn)\n",
    "\n",
    "        # 多层感知器激活，丢弃，全连接层2\n",
    "        x_mlp = self.mlp_act(x_mlp)\n",
    "        x_mlp = self.mlp_drop(x_mlp)\n",
    "        x_mlp = self.mlp_out_proj(x_mlp)\n",
    "\n",
    "        # 添加残差连接、丢弃路径和层缩放应用\n",
    "        y = self.drop_path(self.ls(x_attn + x_mlp))\n",
    "        x = x + y\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013e4d11-e3a1-41d0-b2e4-828531150a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParallelThingsBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim,\n",
    "            num_heads,\n",
    "            num_parallel=2,\n",
    "            mlp_ratio=4.,\n",
    "            qkv_bias=False,\n",
    "            qk_norm=False,\n",
    "            init_values=None,\n",
    "            proj_drop=0.,\n",
    "            attn_drop=0.,\n",
    "            drop_path=0.,\n",
    "            act_layer=nn.GELU,\n",
    "            norm_layer=nn.LayerNorm,\n",
    "            mlp_layer=Mlp,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_parallel = num_parallel\n",
    "        self.attns = nn.ModuleList()\n",
    "        self.ffns = nn.ModuleList()\n",
    "        for _ in range(num_parallel):\n",
    "            self.attns.append(nn.Sequential(OrderedDict([\n",
    "                ('norm', norm_layer(dim)),\n",
    "                ('attn', Attention(\n",
    "                    dim,\n",
    "                    num_heads=num_heads,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    qk_norm=qk_norm,\n",
    "                    attn_drop=attn_drop,\n",
    "                    proj_drop=proj_drop,\n",
    "                    norm_layer=norm_layer,\n",
    "                )),\n",
    "                ('ls', LayerScale(dim, init_values=init_values) if init_values else nn.Identity()),\n",
    "                ('drop_path', DropPath(drop_path) if drop_path > 0. else nn.Identity())\n",
    "            ])))\n",
    "            self.ffns.append(nn.Sequential(OrderedDict([\n",
    "                ('norm', norm_layer(dim)),\n",
    "                ('mlp', mlp_layer(\n",
    "                    dim,\n",
    "                    hidden_features=int(dim * mlp_ratio),\n",
    "                    act_layer=act_layer,\n",
    "                    drop=proj_drop,\n",
    "                )),\n",
    "                ('ls', LayerScale(dim, init_values=init_values) if init_values else nn.Identity()),\n",
    "                ('drop_path', DropPath(drop_path) if drop_path > 0. else nn.Identity())\n",
    "            ])))\n",
    "\n",
    "    def _forward_jit(self, x):\n",
    "        x = x + torch.stack([attn(x) for attn in self.attns]).sum(dim=0)\n",
    "        x = x + torch.stack([ffn(x) for ffn in self.ffns]).sum(dim=0)\n",
    "        return x\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def _forward(self, x):\n",
    "        x = x + sum(attn(x) for attn in self.attns)\n",
    "        x = x + sum(ffn(x) for ffn in self.ffns)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        if torch.jit.is_scripting() or torch.jit.is_tracing():\n",
    "            return self._forward_jit(x)\n",
    "        else:\n",
    "            return self._forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ef8a1c-0181-42c4-b522-6cd37efc3e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vit模块\n",
    "class VisionTransformer(nn.Module):\n",
    "    dynamic_img_size: Final[bool]\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            img_size=224,\n",
    "            patch_size=16,\n",
    "            in_chans: int = 3,\n",
    "            num_classes: int = 1000,\n",
    "            global_pool: str = 'token',\n",
    "            embed_dim: int = 768,\n",
    "            depth: int = 12,\n",
    "            num_heads: int = 12,\n",
    "            mlp_ratio: float = 4.,\n",
    "            qkv_bias: bool = True,\n",
    "            qk_norm: bool = False,\n",
    "            init_values=None,\n",
    "            class_token: bool = True,\n",
    "            no_embed_class: bool = False,\n",
    "            reg_tokens: int = 0,\n",
    "            pre_norm: bool = False,\n",
    "            fc_norm =None,\n",
    "            dynamic_img_size: bool = False,\n",
    "            dynamic_img_pad: bool = False,\n",
    "            drop_rate: float = 0.,\n",
    "            pos_drop_rate: float = 0.,\n",
    "            patch_drop_rate: float = 0.,\n",
    "            proj_drop_rate: float = 0.,\n",
    "            attn_drop_rate: float = 0.,\n",
    "            drop_path_rate: float = 0.,\n",
    "            weight_init: str = '',\n",
    "            embed_layer=PatchEmbed,\n",
    "            norm_layer=None,\n",
    "            act_layer=None,\n",
    "            block_fn=Block,\n",
    "            mlp_layer=Mlp,\n",
    "    ):\n",
    "        \"\"\"\n",
    "            img_size: Input image size.\n",
    "            patch_size: Patch size.\n",
    "            in_chans: Number of image input channels.\n",
    "            num_classes: Mumber of classes for classification head.\n",
    "            global_pool: Type of global pooling for final sequence (default: 'token').\n",
    "            embed_dim: Transformer embedding dimension.\n",
    "            depth: Depth of transformer.\n",
    "            num_heads: Number of attention heads.\n",
    "            mlp_ratio: Ratio of mlp hidden dim to embedding dim.\n",
    "            qkv_bias: Enable bias for qkv projections if True.\n",
    "            init_values: Layer-scale init values (layer-scale enabled if not None).\n",
    "            class_token: Use class token.\n",
    "            no_embed_class: Don't include position embeddings for class (or reg) tokens.\n",
    "            reg_tokens: Number of register tokens.\n",
    "            fc_norm: Pre head norm after pool (instead of before), if None, enabled when global_pool == 'avg'.\n",
    "            drop_rate: Head dropout rate.\n",
    "            pos_drop_rate: Position embedding dropout rate.\n",
    "            attn_drop_rate: Attention dropout rate.\n",
    "            drop_path_rate: Stochastic depth rate.\n",
    "            weight_init: Weight initialization scheme.\n",
    "            embed_layer: Patch embedding layer.\n",
    "            norm_layer: Normalization layer.\n",
    "            act_layer: MLP activation layer.\n",
    "            block_fn: Transformer block layer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert global_pool in ('', 'avg', 'token', 'map')\n",
    "        assert class_token or global_pool != 'token'\n",
    "        use_fc_norm = global_pool == 'avg' if fc_norm is None else fc_norm\n",
    "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "        act_layer = act_layer or nn.GELU\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.global_pool = global_pool\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        self.num_prefix_tokens = 1 if class_token else 0\n",
    "        self.num_prefix_tokens += reg_tokens\n",
    "        self.num_reg_tokens = reg_tokens\n",
    "        self.has_class_token = class_token\n",
    "        self.no_embed_class = no_embed_class  # don't embed prefix positions (includes reg)\n",
    "        self.dynamic_img_size = dynamic_img_size\n",
    "        self.grad_checkpointing = False\n",
    "\n",
    "        embed_args = {}\n",
    "        if dynamic_img_size:\n",
    "            # flatten deferred until after pos embed\n",
    "            embed_args.update(dict(strict_img_size=False, output_fmt='NHWC'))\n",
    "        self.patch_embed = embed_layer(\n",
    "            img_size=img_size,\n",
    "            patch_size=patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim,\n",
    "            bias=not pre_norm,  # disable bias if pre-norm is used (e.g. CLIP)\n",
    "            dynamic_img_pad=dynamic_img_pad,\n",
    "            **embed_args,\n",
    "        )\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if class_token else None\n",
    "        self.reg_token = nn.Parameter(torch.zeros(1, reg_tokens, embed_dim)) if reg_tokens else None\n",
    "        embed_len = num_patches if no_embed_class else num_patches + self.num_prefix_tokens\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, embed_len, embed_dim) * .02)\n",
    "        self.pos_drop = nn.Dropout(p=pos_drop_rate)\n",
    "        if patch_drop_rate > 0:\n",
    "            self.patch_drop = PatchDropout(\n",
    "                patch_drop_rate,\n",
    "                num_prefix_tokens=self.num_prefix_tokens,\n",
    "            )\n",
    "        else:\n",
    "            self.patch_drop = nn.Identity()\n",
    "        self.norm_pre = norm_layer(embed_dim) if pre_norm else nn.Identity()\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            block_fn(\n",
    "                dim=embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                qk_norm=qk_norm,\n",
    "                init_values=init_values,\n",
    "                proj_drop=proj_drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[i],\n",
    "                norm_layer=norm_layer,\n",
    "                act_layer=act_layer,\n",
    "                mlp_layer=mlp_layer,\n",
    "            )\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim) if not use_fc_norm else nn.Identity()\n",
    "\n",
    "        # Classifier Head\n",
    "        if global_pool == 'map':\n",
    "            self.attn_pool = AttentionPoolLatent(\n",
    "                self.embed_dim,\n",
    "                num_heads=num_heads,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                norm_layer=norm_layer,\n",
    "            )\n",
    "        else:\n",
    "            self.attn_pool = None\n",
    "        self.fc_norm = norm_layer(embed_dim) if use_fc_norm else nn.Identity()\n",
    "        self.head_drop = nn.Dropout(drop_rate)\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def _pos_embed(self, x):\n",
    "        if self.dynamic_img_size:\n",
    "            B, H, W, C = x.shape\n",
    "            pos_embed = resample_abs_pos_embed(\n",
    "                self.pos_embed,\n",
    "                (H, W),\n",
    "                num_prefix_tokens=0 if self.no_embed_class else self.num_prefix_tokens,\n",
    "            )\n",
    "            x = x.view(B, -1, C)\n",
    "        else:\n",
    "            pos_embed = self.pos_embed\n",
    "\n",
    "        to_cat = []\n",
    "        if self.cls_token is not None:\n",
    "            to_cat.append(self.cls_token.expand(x.shape[0], -1, -1))\n",
    "        if self.reg_token is not None:\n",
    "            to_cat.append(self.reg_token.expand(x.shape[0], -1, -1))\n",
    "\n",
    "        if self.no_embed_class:\n",
    "            # deit-3, updated JAX (big vision)\n",
    "            # position embedding does not overlap with class token, add then concat\n",
    "            x = x + pos_embed\n",
    "            if to_cat:\n",
    "                x = torch.cat(to_cat + [x], dim=1)\n",
    "        else:\n",
    "            # original timm, JAX, and deit vit impl\n",
    "            # pos_embed has entry for class token, concat then add\n",
    "            if to_cat:\n",
    "                x = torch.cat(to_cat + [x], dim=1)\n",
    "            x = x + pos_embed\n",
    "\n",
    "        return self.pos_drop(x)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = self._pos_embed(x)\n",
    "        x = self.patch_drop(x)\n",
    "        x = self.norm_pre(x)\n",
    "        if self.grad_checkpointing and not torch.jit.is_scripting():\n",
    "            x = checkpoint_seq(self.blocks, x)\n",
    "        else:\n",
    "            x = self.blocks(x)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    def forward_head(self, x, pre_logits: bool = False):\n",
    "        if self.attn_pool is not None:\n",
    "            x = self.attn_pool(x)\n",
    "        elif self.global_pool == 'avg':\n",
    "            x = x[:, self.num_prefix_tokens:].mean(dim=1)\n",
    "        elif self.global_pool:\n",
    "            x = x[:, 0]  # class token\n",
    "        x = self.fc_norm(x)\n",
    "        x = self.head_drop(x)\n",
    "        return x if pre_logits else self.head(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.forward_head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd17cca7-3e40-4285-ba21-439d248edb2b",
   "metadata": {},
   "source": [
    "评估函数模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a0f3387-b21a-4420-99cb-11d98e0c65be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#精度评估函数\n",
    "def evaluate_accuracy_gpu(net, data_iter, device=None):\n",
    "    if isinstance(net, nn.Module):\n",
    "        net.eval()\n",
    "        if not device:\n",
    "            device = next(iter(net.parameters())).device\n",
    "    metric = d2l.Accumulator(2)\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            if isinstance(X, list):\n",
    "                X = [x.to(device) for x in X]\n",
    "            else:\n",
    "                X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            metric.add(d2l.accuracy(net(X), y), y.numel())\n",
    "    return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0328c8b6-cc45-4d5b-9a49-47be1c8e421c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#绘制多分类ROC曲线\n",
    "def plot_multiclass_roc_curve(model, test_iter):\n",
    "    # 获取模型的预测概率和真实标签\n",
    "    y_true = []\n",
    "    y_prob = []\n",
    "    for batch in test_iter:\n",
    "        inputs, labels = batch\n",
    "        inputs = inputs.cuda()\n",
    "        outputs = model(inputs)\n",
    "        probabilities = outputs.softmax(dim=1)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_prob.extend(probabilities.cpu().detach().numpy())\n",
    "    y_true = np.array(y_true)\n",
    "    y_prob = np.array(y_prob)\n",
    "    y_prob = np.argmax(y_prob,axis=1)\n",
    "\n",
    "    #微平均和宏平均方法（Micro-average and Macro-average）\n",
    "    # 将真实标签转换为二值矩阵\n",
    "    y_true = label_binarize(y_true, classes=np.unique(y_true))\n",
    "    y_prob = label_binarize(y_prob, classes=np.unique(y_prob))\n",
    "\n",
    "    # 将多分类问题转换为二分类问题\n",
    "    y_true = y_true.ravel()\n",
    "    y_prob = y_prob.ravel()\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # 绘制ROC曲线\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfff865-7d95-4580-ab5c-aa41f5ad807f",
   "metadata": {},
   "source": [
    "训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dab7131-5ca1-4f3b-a2d7-c199fd5d5966",
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练函数\n",
    "def train(net, train_iter, test_iter, num_epochs, lr, device):\n",
    "    def init_weights(m):\n",
    "        if type(m) == nn.Linear or type(m) == nn.Conv2d:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "    net.apply(init_weights)\n",
    "    print('training on', device)\n",
    "    net.to(device)\n",
    "    optimizer = Adam(net.parameters(), lr)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    animator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n",
    "                            legend=['train loss', 'train acc', 'test acc'])\n",
    "    timer, num_batches = d2l.Timer(), len(train_iter)\n",
    "    for epoch in range(num_epochs):\n",
    "        metric = d2l.Accumulator(3)\n",
    "        net.train()\n",
    "        for i, (X, y) in enumerate(train_iter):\n",
    "            timer.start()\n",
    "            optimizer.zero_grad()\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                metric.add(l * X.shape[0], d2l.accuracy(y_hat, y), X.shape[0])\n",
    "            timer.stop()\n",
    "            train_l = metric[0] / metric[2]\n",
    "            train_acc = metric[1] / metric[2]\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i + 1) / num_batches,\n",
    "                             (train_l, train_acc, None))\n",
    "        test_acc = evaluate_accuracy_gpu(net, test_iter)\n",
    "        animator.add(epoch + 1, (None, None, test_acc))\n",
    "    print(f'loss {train_l:.3f}, train acc {train_acc:.3f}, '\n",
    "          f'test acc {test_acc:.3f}')\n",
    "    print(f'{metric[2] * num_epochs / timer.sum():.1f} examples/sec '\n",
    "          f'on {str(device)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bccffac-0439-4850-b7b2-3151d0693cac",
   "metadata": {},
   "source": [
    "主模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97758c77-ff32-4c68-b6e2-79253a779e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #数据处理\n",
    "    train_path = data_dir + '/training'\n",
    "    test_path = data_dir + '/testing'\n",
    "    \n",
    "    train_data = torchvision.datasets.ImageFolder(train_path,transform=transform)\n",
    "    test_data = torchvision.datasets.ImageFolder(test_path,transform=transform)\n",
    "\n",
    "    train_iter=torch.utils.data.DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
    "    test_iter=torch.utils.data.DataLoader(test_data,batch_size=batch_size,shuffle=True)\n",
    "\n",
    "    #训练\n",
    "    train(model, train_iter, test_iter, num_epochs, lr, device)\n",
    "    \n",
    "    #保存模型\n",
    "    torch.save(model,save_dir)\n",
    "\n",
    "    #调用模型计算ROC曲线\n",
    "    plot_multiclass_roc_curve(model, test_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f470563-3dea-471a-bf2f-6475d7fe8f2b",
   "metadata": {},
   "source": [
    "参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8fc3aa2-db4b-4257-89d6-93a79b1c6324",
   "metadata": {},
   "outputs": [],
   "source": [
    "#设置参数\n",
    "device = d2l.try_gpu()#设置运行模式：CPU/GPU\n",
    "batch_size, lr, num_epochs = 20, 0.0001, 100#设置批量、学习率和迭代次数\n",
    "data_dir = '../../数据/'#设置文件读取路径\n",
    "save_dir = './model_.pt'#设置保存路径\n",
    "image_size = 224#将所有图片统一转换的大小\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.Resize((image_size, image_size)),\n",
    "                                            torchvision.transforms.ToTensor(),\n",
    "                                            torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "patch_size, dim, depth, heads, mlp_dim = 32, 1536, 12, 12, 2048#保证patch_size可整除image_size\n",
    "#在网络参数方面，使用了1536个单元的维度，12个 Transformer 块的深度，12个 Transformer 头，MLP 使用2048维度\n",
    "num_classes = 3#输出类别数量\n",
    "\n",
    "#设置随机数种子\n",
    "seed = 0\n",
    "\n",
    "#定义模型和训练模块\n",
    "model = VisionTransformer(img_size=image_size,\n",
    "                          patch_size=patch_size,\n",
    "                          num_classes=num_classes,\n",
    "                          depth=depth,\n",
    "                          num_heads=heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f5cbd5-f32a-450a-bc30-8649f5484d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
