{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入所需的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 公共库\n",
    "import os\n",
    "import sys\n",
    "from ast import literal_eval\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "# 词元化所需\n",
    "from transformers import BertTokenizer\n",
    "# 训练所需\n",
    "import time\n",
    "from collections import defaultdict\n",
    "# 架构所需\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import GPT2LMHeadModel\n",
    "# 配置所需\n",
    "import random\n",
    "import numpy as np\n",
    "# 主函数所需\n",
    "from torch.utils.data import Dataset\n",
    "import tqdm.notebook as tq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本函数和配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CfgNode:\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self._str_helper(0)\n",
    "\n",
    "    def _str_helper(self, indent):\n",
    "        parts = []\n",
    "        for k, v in self.__dict__.items():\n",
    "            if isinstance(v, CfgNode):\n",
    "                parts.append(\"%s:\\n\" % k)\n",
    "                parts.append(v._str_helper(indent + 1))\n",
    "            else:\n",
    "                parts.append(\"%s: %s\\n\" % (k, v))\n",
    "        parts = [' ' * (indent * 4) + p for p in parts]\n",
    "        return \"\".join(parts)\n",
    "\n",
    "    def to_dict(self):\n",
    "        return { k: v.to_dict() if isinstance(v, CfgNode) else v for k, v in self.__dict__.items() }\n",
    "\n",
    "    def merge_from_dict(self, d):\n",
    "        self.__dict__.update(d)\n",
    "\n",
    "    def merge_from_args(self, args):\n",
    "        for arg in args:\n",
    "\n",
    "            keyval = arg.split('=')\n",
    "            assert len(keyval) == 2, \"expecting each override arg to be of form --arg=value, got %s\" % arg\n",
    "            key, val = keyval\n",
    "            try:\n",
    "                val = literal_eval(val)\n",
    "            except ValueError:\n",
    "                pass\n",
    "            assert key[:2] == '--'\n",
    "            key = key[2:]\n",
    "            keys = key.split('.')\n",
    "            obj = self\n",
    "            for k in keys[:-1]:\n",
    "                obj = getattr(obj, k)\n",
    "            leaf_key = keys[-1]\n",
    "\n",
    "            assert hasattr(obj, leaf_key), f\"{key} is not an attribute that exists in the config\"\n",
    "\n",
    "            print(\"command line overwriting config attribute %s with %s\" % (key, val))\n",
    "            setattr(obj, leaf_key, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewGELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # 判断嵌入维度是否可以整除头的个数 防止出现浮点数\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # 这个线性层输入维度是单个输入向量的维度，\n",
    "        # 输出是3 * 输入向量维度是为了一次性将Q K V计算出来, 参见forward中的计算\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)# c_attn有所不同\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "\n",
    "        # 设置dropout\n",
    "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
    "\n",
    "        # 注册两个tensor，并且这些tensor在训练过程中不会更新\n",
    "        # 第一个是实现mask机制的掩码矩阵\n",
    "        # 第二个在transformers源码中没有找到显式的使用， 猜测可能是一种实现mask策略的常数偏置\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                     .view(1, 1, config.block_size, config.block_size))\n",
    "        '''\n",
    "        注册一个tensor为模型的一部分，并且这个tensor不会被当作模型参数来处理，类似于一个常量。\n",
    "        '''\n",
    "        # 根据transformers源码增加的修复\n",
    "        #self.register_buffer(\"masked_bias\", torch.tensor(-1e4), persistent=False)\n",
    "\n",
    "        # 头的个数和嵌入空间的维度\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 获取batch size, sequence length, 模型维度\n",
    "        B, T, C = x.size() \n",
    "        # 通过c_attn一次性计算q, k, v并按head数进行分割\n",
    "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        # 三个向量的形状处理为(B, nh, T, hs)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) \n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        \n",
    "        # scaled dot product\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        \n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        # 获得最终得分\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Layernorm其一\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        # 掩码注意力层 Masked Multi-Head Attention\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        # Layernorm其二\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        # FFN层\n",
    "        self.mlp = nn.ModuleDict(dict(\n",
    "            c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            c_proj  = nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            act     = NewGELU(),\n",
    "            dropout = nn.Dropout(config.resid_pdrop),\n",
    "        ))\n",
    "        m = self.mlp\n",
    "        # 映射到4 * 模型维度-GELU激活-再映射回模型维度-dropout\n",
    "        # self.mlpf是一个函数对象, 而不是一个变量对象 \n",
    "        self.mlpf = lambda x: m.dropout(m.c_proj(m.act(m.c_fc(x))))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlpf(self.ln_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    @staticmethod\n",
    "    def get_default_config():\n",
    "        C = CfgNode()\n",
    "        # 配置中必须给出 model_type 或 （n_layer， n_head， n_embd）\n",
    "        C.model_type = 'gpt'\n",
    "        C.n_layer = None\n",
    "        C.n_head = None\n",
    "        C.n_embd =  None\n",
    "        # 这些选项必须在外部填写\n",
    "        C.vocab_size = None\n",
    "        C.block_size = None\n",
    "        # Dropout 超参数\n",
    "        C.embd_pdrop = 0.1\n",
    "        C.resid_pdrop = 0.1\n",
    "        C.attn_pdrop = 0.1\n",
    "        return C\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # 第一步, 我们判断vocab_size和block_size不为空\n",
    "        # assert的作用是用来判断后面的布尔表达式\n",
    "        # 如果真则没影响, 否则会raise一个Attribute Error(和try exception有点类似)\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.block_size = config.block_size\n",
    "\n",
    "        type_given = config.model_type is not None\n",
    "        # 判断是否指定模型预设类型\n",
    "        params_given = all([config.n_layer is not None, config.n_head is not None, config.n_embd is not None])\n",
    "        # 如果config中既给了layer, head又给了embd数, 说明这是一个自定义的参数设置\n",
    "        assert type_given ^ params_given\n",
    "        # 异或, 不能既使用指定预设,又给具体的参数, 这样有冲突\n",
    "        if type_given:\n",
    "            # 将model_type转换为详细配置\n",
    "            config.merge_from_dict({\n",
    "                # GPT-1\n",
    "                'openai-gpt':   dict(n_layer=12, n_head=12, n_embd=768),  # 117M params\n",
    "                # GPT-2 configs\n",
    "                'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "                'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "                'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "                'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "                # Gophers\n",
    "                'gopher-44m':   dict(n_layer=8, n_head=16, n_embd=512),\n",
    "                # 超小模型\n",
    "                'gpt-mini':     dict(n_layer=6, n_head=6, n_embd=192),\n",
    "                'gpt-micro':    dict(n_layer=4, n_head=4, n_embd=128),\n",
    "                'gpt-nano':     dict(n_layer=3, n_head=3, n_embd=48),\n",
    "            }[config.model_type])\n",
    "\n",
    "        '''\n",
    "        vocab_size(词表大小)\n",
    "        n_embd(模型的维度)\n",
    "        block_size(输入的长度)\n",
    "        embd_pdrop(嵌入层的dropout比率)\n",
    "        n_layer(block的个数)\n",
    "        '''\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            # word token Embedding, 用来词嵌入\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            # word Positional Embedding 用来位置嵌入\n",
    "            drop = nn.Dropout(config.embd_pdrop),\n",
    "            # dropout设置\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            # 堆叠Decoder layer或者说Block\n",
    "            ln_f = nn.LayerNorm(config.n_embd)))\n",
    "            # Layernorm层\n",
    "        \n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        # 将预测向量映射回词典\n",
    "\n",
    "        # 根据 GPT-2 论文，init 所有权重，并将特殊缩放的 init 应用于残差投影\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "        '''\n",
    "        c_proj是每个Attention的最后一个线性层, 还有一个c_proj是FFN(也就是self.MLP)中的c_proj, \n",
    "        也就是说在一个Block中会进行两次的Layernorm操作. 因此考虑到多个LayerNorm层对权重和梯度的累积效果。\n",
    "        通过调整c_proj.weight的初始化，可能可以更好地平衡这种累积效果，从而帮助模型更稳定地训练。\n",
    "        '''\n",
    "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
    "        print(\"number of parameters: %.2fM\" % (n_params/1e6,))\n",
    "        # 输出参数量\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        # isinstance用于判断第一个参数是否是第二个参数的实例\n",
    "        # 对Linear层初始化\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # 对线形层初始化\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            # 对Embedding层初始化\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            # 对LayerNorm层初始化\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type):\n",
    "        '''\n",
    "        通过从 huggingface/transformers 检查点复制权重来初始化预训练的 GPT 模型。\n",
    "        '''\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "\n",
    "        # 从头开始创建初始化的 minGPT 模型\n",
    "        config = cls.get_default_config()\n",
    "        config.model_type = model_type\n",
    "        config.vocab_size = 50257 # openai's model vocabulary\n",
    "        config.block_size = 1024  # openai's model block_size\n",
    "        model = GPT(config)\n",
    "        # 初始化model\n",
    "        sd = model.state_dict()\n",
    "        # 获取参数字典\n",
    "\n",
    "        # 从hugging face中加载一个训练好的模型\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        # 获取参数字典\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # 确保所有参数在名称和形状上对齐并匹配\n",
    "        keys = [k for k in sd_hf if not k.endswith('attn.masked_bias')]\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # 判断长度是否相同\n",
    "        for k in keys:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # 由于hugging face使用的线性层是自定义的Conv1D，对我们需要转置的 Conv1D 权重进行特殊处理\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # 正常复制其他参数\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def configure_optimizers(self, train_config):\n",
    "        '''\n",
    "        将模型的所有参数分成两个部分：将经历正则化的权重衰减和不正则化的权重衰减（偏差和层规范/嵌入权重）。\n",
    "        weight decay防止过拟合，但是不是所有参数都需要weight decay, 因此对Optimizer进行预先配置。\n",
    "        weight decay鼓励模型变得简单, 参数接近0, 而dropout通过关闭一些节点,让模型变得略有不同,防止模型过于依赖某些节点, 通常都会一起使用。\n",
    "        '''\n",
    "        # 需要weight decay以及不需要的参数集合\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        # 通常来说我们只对线性层做weight decay, 主要是Attention\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "        # 遍历模型参数, mn是Moudle name m是Moudle\n",
    "            for pn, p in m.named_parameters():\n",
    "            # 参数名, 参数\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # 完整的参数名称\n",
    "                if pn.endswith('bias'):\n",
    "                    # bias是不需要weight decay的\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                # 白名单模块的权重将进行权重衰减（判断是否在白名单中, 也就是判断它是否是Linear类）\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                # 黑名单模块的权重不会衰减\n",
    "                    no_decay.add(fpn)\n",
    "\n",
    "        # 判断有没有参数既被判定需要weight decay又被判定不需要(同时存在于两个集合当中)\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # 创建 PyTorch 优化器对象\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0}]\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        # 判断一下输入的tensor有没有超出最大长度\n",
    "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
    "        \n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # 形状是(1, t)\n",
    "        tok_emb = self.transformer.wte(idx) # 获取token embeddings (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # 获取position embeddings (1, t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # logits是一个长度为token词表大小, 对应位置是输出这个词的概率\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):\n",
    "        # idx仍然是原始输入的序列, 在这里idx会被叫做原始输入, 而输入默认指代idx_cond\n",
    "        # max_new_token是我们设定的本次生成时最多生成的长度\n",
    "        # temperature是温度, 用于控制生成的多样性\n",
    "        # do_sample是是否启用多项式生成策略\n",
    "        # top_k也是一种生成策略\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n",
    "            # 如果随着输入的增长超出了模型的最大序列长就做截断, 只要倒数self.block_size个\n",
    "            logits, _ = self(idx_cond)\n",
    "            # 然后我们获取logits, _是loss但是在generate过程中不太需要,所以没有特意命名\n",
    "            # slef的调用类似于在外部直接调用model(input),也是调用了forward方法\n",
    "            # 在最后一步输出对数，并按所需temperature缩放\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # （可选）将 logits 裁剪为仅前 k 个选项\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, top_k)\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # 应用 SoftMax 将 logits 转换为（归一化）概率\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # 要么从分布中抽样，要么选取最可能的元素\n",
    "            if do_sample:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # 否则，选择最可能的 token\n",
    "            else:\n",
    "                _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
    "            # 将采样索引追加到运行序列并继续\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx[:,idx_len:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    @staticmethod\n",
    "    def get_default_config():\n",
    "        C = CfgNode()\n",
    "        C.device = 'auto'\n",
    "        C.num_epochs = 100\n",
    "        C.learning_rate = 3e-4\n",
    "        C.betas = (0.9, 0.95)\n",
    "        C.weight_decay = 0.1\n",
    "        C.grad_norm_clip = 1.0\n",
    "        return C\n",
    "\n",
    "    def __init__(self, config, model, train_loader):\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.optimizer = None\n",
    "        self.train_loader = train_loader\n",
    "\n",
    "        if config.device == 'auto':\n",
    "            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        else:\n",
    "            self.device = config.device\n",
    "        self.model = self.model.to(self.device)\n",
    "        print(\"running on device\", self.device)\n",
    "\n",
    "        self.count = 0\n",
    "        self.iter_time = 0.0\n",
    "        self.iter_dt = 0.0\n",
    "        self.sum_loss = 0.0\n",
    "\n",
    "    def run(self):\n",
    "        model, config = self.model, self.config # 初始化模型和配置\n",
    "        self.optimizer = model.configure_optimizers(config) # 初始化优化器\n",
    "\n",
    "        for epoch in range(1,config.num_epochs+1):\n",
    "            model.train() # 模型设置为训练模式(会正常使用dropout等机制)\n",
    "            self.sum_loss = 0.0\n",
    "            self.count\n",
    "            self.iter_time = time.time()\n",
    "\n",
    "            for i, (x, y) in enumerate(tq.tqdm(self.train_loader)):\n",
    "                samples = x.shape[0]\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                logits, self.loss = model(x, y)\n",
    "                model.zero_grad(set_to_none=True)\n",
    "                self.loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
    "                self.optimizer.step()\n",
    "                self.sum_loss += self.loss.item() * samples\n",
    "                self.count += samples\n",
    "                if ((i+1) % 1000 == 0 and i!=0) or i==len(train_loader)-1:\n",
    "                    tnow = time.time()\n",
    "                    self.iter_dt = tnow - self.iter_time\n",
    "                    self.iter_time = tnow\n",
    "                    f = open(loggin_dir, \"a\")\n",
    "                    f.write(\"Epoch [{}][{}/{}]\\tLoss: {:.5f}  time: {:.2f}\".format(epoch, i+1, len(train_loader),\n",
    "                                                                                   self.sum_loss/self.count,\n",
    "                                                                                   self.iter_dt)+'s'+'\\n')\n",
    "                    print(\"Epoch [{}][{}/{}]\\tLoss: {:.5f}  time: {:.2f}\".format(epoch, i+1, len(train_loader),\n",
    "                                                                                 self.sum_loss/self.count,\n",
    "                                                                                 self.iter_dt)+'s')\n",
    "                    f.close()\n",
    "                    \n",
    "            torch.save(model.state_dict(), save_dir+str(epoch)+'.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集制作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, data, max_len, vocab_size):\n",
    "        self.data = data\n",
    "        self.max_len = max_len\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size\n",
    "    \n",
    "    def get_block_size(self):\n",
    "        # 馈入transformer的序列长度, 包含串联的输入和输出，但 -1，因为transformer在最后一个输入元素处开始进行预测\n",
    "        return self.max_len * 2 - 1\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        question = torch.LongTensor(self.data[i][0])\n",
    "        reply = torch.LongTensor(self.data[i][1])\n",
    "\n",
    "        # 将问题规范和解决方案连接起来\n",
    "        cat = torch.cat((question, reply), dim=0)\n",
    "\n",
    "        # transformer的输入将是偏移序列\n",
    "        x = cat[:-1].clone()\n",
    "        y = cat[1:].clone()\n",
    "        # 在输出位置进行预测，在输入位置掩盖损失\n",
    "        y[:self.max_len-1] = -1\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机种子设置：3407或42\n",
    "seed = 3407\n",
    "\n",
    "# 路径设置\n",
    "data_dir = './'# 预处理数据路径\n",
    "bert_path = '../tokenizer'# tokenizer路径\n",
    "save_dir = './checkpoint/checkpoint_'# 模型参数保存路径\n",
    "loggin_dir = \"training_data.txt\"# 训练结果日志\n",
    "\n",
    "# 数据集制作参数\n",
    "batch_size = 16\n",
    "num_workers = 0\n",
    "max_len = 200\n",
    "\n",
    "# 模型参数设置\n",
    "n_layer = 6\n",
    "n_head = 8\n",
    "n_embd = 512\n",
    "\n",
    "# 训练参数设置\n",
    "num_epochs = 1000\n",
    "learning_rate = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_dir, 'r', encoding='gb18030') as file:\n",
    "    pairs = []\n",
    "    for line in file:\n",
    "        pair = eval(line)\n",
    "        pairs.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words are: 21128\n"
     ]
    }
   ],
   "source": [
    "# 创建Bert分词器\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_path)\n",
    "# 读取字典信息\n",
    "word_map = tokenizer.get_vocab()\n",
    "print(\"Total words are: {}\".format(len(word_map)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(data=pairs, max_len=max_len, vocab_size=len(word_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 29.94M\n"
     ]
    }
   ],
   "source": [
    "set_seed(seed)\n",
    "model_config = GPT.get_default_config()\n",
    "model_config.model_type = None\n",
    "model_config.n_layer = n_layer\n",
    "model_config.n_head = n_head\n",
    "model_config.n_embd = n_embd\n",
    "model_config.vocab_size = train_dataset.get_vocab_size()\n",
    "model_config.block_size = train_dataset.get_block_size()\n",
    "model = GPT(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "超参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device cuda\n"
     ]
    }
   ],
   "source": [
    "train_config = Trainer.get_default_config()\n",
    "train_config.num_epochs = num_epochs\n",
    "train_config.learning_rate = learning_rate\n",
    "trainer = Trainer(train_config, model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c4200963abf4b20849e9b6ebdc7cf2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1][1000/3225]\tLoss: 2.570  time: 305.08s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 51\u001b[0m, in \u001b[0;36mTrainer.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     49\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), config\u001b[38;5;241m.\u001b[39mgrad_norm_clip)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msum_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m samples\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcount \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m samples\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m i\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m i\u001b[38;5;241m==\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
