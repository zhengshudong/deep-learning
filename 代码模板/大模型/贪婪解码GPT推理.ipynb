{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "851f05b2-b2da-46e5-9cec-457d588f6121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from transformers import BertTokenizer\n",
    "bert_path = '../tokenizer/models--bert-base-chinese/snapshots/8d2a91f91cc38c96bb8b4556ba70c392f8d5ee55'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_path)\n",
    "word_map = tokenizer.get_vocab()\n",
    "max_len = 200\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len, num_layers = 6):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pe = self.create_positinal_encoding(max_len, self.d_model)\n",
    "        self.te = self.create_positinal_encoding(num_layers, self.d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    def create_positinal_encoding(self, max_len, d_model):\n",
    "        pe = torch.zeros(max_len, d_model).to(device)\n",
    "        for pos in range(max_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        return pe\n",
    "    def forward(self, embedding, layer_idx):\n",
    "        if layer_idx == 0:\n",
    "            embedding = self.embed(embedding) * math.sqrt(self.d_model)\n",
    "        embedding += self.pe[:, :embedding.size(1)]\n",
    "        embedding += self.te[:, layer_idx, :].unsqueeze(1).repeat(1, embedding.size(1), 1)\n",
    "        embedding = self.dropout(embedding)\n",
    "        return embedding\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % heads == 0\n",
    "        self.d_k = d_model // heads\n",
    "        self.heads = heads\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        self.concat = nn.Linear(d_model, d_model)\n",
    "    def forward(self, query, key, value, mask):\n",
    "        query = self.query(query)\n",
    "        key = self.key(key)        \n",
    "        value = self.value(value)   \n",
    "        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)   \n",
    "        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
    "        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
    "        scores = torch.matmul(query, key.permute(0,1,3,2)) / math.sqrt(query.size(-1))\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        weights = F.softmax(scores, dim = -1)\n",
    "        weights = self.dropout(weights)\n",
    "        context = torch.matmul(weights, value)\n",
    "        context = context.permute(0,2,1,3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n",
    "        interacted = self.concat(context)\n",
    "        return interacted \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, middle_dim = 2048):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, middle_dim)\n",
    "        self.fc2 = nn.Linear(middle_dim, d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.fc2(self.dropout(out))\n",
    "        return out\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    def forward(self, embeddings, mask):\n",
    "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
    "        interacted = self.layernorm(interacted + embeddings)\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        encoded = self.layernorm(feed_forward_out + interacted)\n",
    "        return encoded\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
    "        self.src_multihead = MultiHeadAttention(heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    def forward(self, embeddings, encoded, src_mask, target_mask):\n",
    "        query = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, target_mask))\n",
    "        query = self.layernorm(query + embeddings)\n",
    "        interacted = self.dropout(self.src_multihead(query, encoded, encoded, src_mask))\n",
    "        interacted = self.layernorm(interacted + query)\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        decoded = self.layernorm(feed_forward_out + interacted)\n",
    "        return decoded\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, heads, num_layers, word_map, max_len):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_size = len(word_map)\n",
    "        self.embed = Embeddings(self.vocab_size, d_model, num_layers = num_layers)\n",
    "        self.encoder = EncoderLayer(d_model, heads) \n",
    "        self.decoder = DecoderLayer(d_model, heads)\n",
    "        self.logit = nn.Linear(d_model, self.vocab_size)\n",
    "    def encode(self, src_embeddings, src_mask):\n",
    "        for i in range(self.num_layers):\n",
    "            src_embeddings = self.embed(src_embeddings, i)\n",
    "            src_embeddings = self.encoder(src_embeddings, src_mask)\n",
    "        return src_embeddings\n",
    "    def decode(self, tgt_embeddings, target_mask, src_embeddings, src_mask):\n",
    "        for i in range(self.num_layers):\n",
    "            tgt_embeddings = self.embed(tgt_embeddings, i)\n",
    "            tgt_embeddings = self.decoder(tgt_embeddings, src_embeddings, src_mask, target_mask)\n",
    "        return tgt_embeddings\n",
    "    def forward(self, src_words, src_mask, target_words, target_mask):\n",
    "        encoded = self.encode(src_words, src_mask)\n",
    "        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n",
    "        out = F.log_softmax(self.logit(decoded), dim = 2)\n",
    "        return out\n",
    "class AdamWarmup:\n",
    "    def __init__(self, model_size, warmup_steps, optimizer):\n",
    "        self.model_size = model_size\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.optimizer = optimizer\n",
    "        self.current_step = 0\n",
    "        self.lr = 0\n",
    "    def get_lr(self):\n",
    "        return self.model_size ** (-0.5) * min(self.current_step ** (-0.5), self.current_step * self.warmup_steps ** (-1.5))\n",
    "    def step(self):\n",
    "        self.current_step += 1\n",
    "        lr = self.get_lr()\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        self.lr = lr\n",
    "        self.optimizer.step()\n",
    "def evaluate(transformer, question, question_mask, max_len, word_map):\n",
    "    rev_word_map = {v: k for k, v in word_map.items()}\n",
    "    transformer.eval()\n",
    "    start_token = word_map['[CLS]']\n",
    "    encoded = transformer.encode(question, question_mask)\n",
    "    words = torch.LongTensor([[start_token]]).to(device)\n",
    "    for step in range(max_len - 1):\n",
    "        size = words.shape[1]\n",
    "        target_mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
    "        target_mask = target_mask.to(device).unsqueeze(0).unsqueeze(0)\n",
    "        decoded = transformer.decode(words, target_mask, encoded, question_mask)\n",
    "        predictions = transformer.logit(decoded[:, -1])\n",
    "        _, next_word = torch.max(predictions, dim = 1)\n",
    "        next_word = next_word.item()\n",
    "        if next_word == word_map['[SEP]']:\n",
    "            break\n",
    "        words = torch.cat([words, torch.LongTensor([[next_word]]).to(device)], dim = 1)\n",
    "    if words.dim() == 2:\n",
    "        words = words.squeeze(0)\n",
    "        words = words.tolist()\n",
    "    sen_idx = [w for w in words if w not in {word_map['[CLS]']}]\n",
    "    sentence = ' '.join([rev_word_map[sen_idx[k]] for k in range(len(sen_idx))])\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "994d5e8d-6d5e-4a2a-9afb-922cb00e47df",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('../checkpoint/checkpoint_total.pth.tar')\n",
    "transformer = checkpoint['transformer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c46c33e8-8e5c-414a-9f2e-dc495d05309d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">>>User:  房事之后很累怎么办\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 你 好 ， 根 据 你 的 叙 述 ， 考 虑 是 炎 症 引 起 的 ， 一 般 和 局 部 病 原 菌 感 染 有 关 系 的 ， 可 以 口 服 阿 莫 灵 ， 甲 硝 唑 治 疗 试 试 ， 注 意 休 息 ， 不 要 刺 激 性 食 物 ， 多 喝 水 ， 慢 慢 会 改 善 的\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">>>User:  肛门裂开了怎么办\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 你 好 ， 肛 裂 是 肛 门 直 肠 粘 膜 下 静 脉 丛 发 生 扩 张 而 形 成 的 柔 软 静 脉 团 ， 可 以 用 无 花 果 叶 子 煎 水 熏 洗 治 疗 ， 有 一 定 效 果\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">>>User:  我男的，和老婆房事完很累\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 你 好 ， 这 种 情 况 可 以 应 用 中 医 六 味 地 黄 丸 、 男 宝 胶 囊 等 药 物 治 疗 缓 解 病 情 。 平 时 可 以 吃 些 胡 桃 、 栗 子 、 猪 腰 、 山 药 、 枸 杞 、 羊 肉 、 韭 菜 、 西 红 柿 等 食 物 调 理 看 。 平 时 多 锻 炼 身 体 增 强 体 质 有 助 缓 解 病 情 。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">>>User:  我女的，阴道很痒\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 你 好 ， 根 据 你 的 叙 述 ， 考 虑 是 炎 症 引 起 的 ， 一 般 和 局 部 病 原 菌 感 染 有 关 系 的 ， 可 以 口 服 阿 莫 灵 ， 甲 硝 唑 治 疗 试 试 ， 注 意 休 息 ， 不 要 刺 激 性 食 物 ， 多 喝 水 ， 慢 慢 会 改 善 的\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">>>User:  我女的，阴道炎\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 你 好 ， 这 种 情 况 应 该 是 阴 道 炎 或 宫 颈 炎 等 原 因 造 成 的 ， 可 以 应 用 氧 氟 沙 星 或 罗 红 霉 素 等 抗 菌 药 物 治 疗 看 ， 平 时 注 意 保 持 卫 生 习 惯 ， 避 免 刺 激 性 食 物 。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">>>User:  脑子里面有个肿瘤怎么办\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 你 好 ， 根 据 你 的 叙 述 ， 考 虑 是 恶 性 肿 瘤 ， 一 般 可 以 化 疗 或 放 疗 ， 同 时 用 中 药 治 疗 ， 延 长 病 人 生 命 ， 减 轻 痛 苦 ， 除 了 正 规 治 疗 脑 瘤 外 ， 患 者 需 要 多 咨 询 专 家 建 议 ， 和 医 生 保 持 沟 通 ， 患 者 还 需 要 重 视 饮 食 方 面 ， 例 如 饮 食 清 淡 ， 避 免 辛 辣 刺 激 食 物 。 与 此 同 时 患 者 还 要 注 意 选 择 一 家 正 规 医 院 诊 治 ， 这 样 才 能 得 到 良 好 的 治 疗 效 果 。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">>>User:  胰腺癌会不会死啊\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 胰 腺 癌 的 治 疗 方 法 有 手 术 治 疗 、 化 疗 、 放 疗 、 中 药 治 疗 、 中 药 治 疗 等 。 建 议 您 到 正 规 的 医 院 做 个 详 细 的 检 查 ， 明 确 诊 断 后 再 对 症 治 疗 。 ， 等 到 胰 腺 癌 病 情 得 到 改 善 的 时 候 ， 患 者 切 忌 盲 目 饮 食 ， 最 好 以 清 淡 食 物 为 主 ， 如 果 选 择 了 手 术 治 疗 ， 术 后 一 定 要 重 视 自 身 的 护 理 工 作 ， 并 且 注 意 补 充 维 生 素 ， 多 吃 清 淡 的 食 物 ， 希 望 患 者 病 情 可 以 得 到 缓 解 。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">>>User:  呼吸困难，走路没力气\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e: 你 好 ， 这 种 情 况 应 该 是 有 心 脏 病 或 肺 部 疾 病 引 起 的 ， 可 以 应 用 中 医 六 味 地 黄 丸 、 心 得 安 、 谷 维 素 等 药 物 治 疗 看 。 平 时 注 意 保 持 良 好 的 饮 食 及 卫 生 习 惯 。 避 免 刺 激 性 食 物 。\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">>>User:  quit\n"
     ]
    }
   ],
   "source": [
    "while(1):\n",
    "    question = input(\">>>User: \") \n",
    "    if question == 'quit':\n",
    "        break\n",
    "    encoded_question = tokenizer.encode_plus(question,return_tensors=\"pt\")\n",
    "    question_ids = encoded_question[\"input_ids\"]\n",
    "    enc_qus = question_ids[0].tolist()\n",
    "    question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n",
    "    question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1) \n",
    "    sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n",
    "    print(\"e: \"+sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
